{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCED FROM: https://github.com/DimaKrotov/Biological_Learning\n",
    "# To draw a heatmap of the weights a helper function is created\n",
    "\n",
    "def draw_weights(synapses, Kx, Ky):\n",
    "    yy=0\n",
    "    HM=np.zeros((28*Ky,28*Kx))\n",
    "    for y in range(Ky):\n",
    "        for x in range(Kx):\n",
    "            HM[y*28:(y+1)*28,x*28:(x+1)*28]=synapses[yy,:].reshape(28,28)\n",
    "            yy += 1\n",
    "    plt.clf()\n",
    "    nc=np.amax(np.absolute(HM))\n",
    "    im=plt.imshow(HM,cmap='bwr',vmin=-nc,vmax=nc)\n",
    "    fig.colorbar(im,ticks=[np.amin(HM), 0, np.amax(HM)])\n",
    "    plt.axis('off')\n",
    "    fig.canvas.draw() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables required for the algorithm\n",
    "\n",
    "learning_rate = 2e-2    # initial learning rate\n",
    "Kx = 10\n",
    "Ky = 10\n",
    "n_hidden = Kx*Ky        # number of hidden units that are displayed in Ky by Kx array\n",
    "mu = 0.0                # mean for gaussian distribution to initialize weights with\n",
    "sigma = 1.0             # standard deviation for gaussian distribution to initialize weights with\n",
    "n_epochs = 200          # number of epochs\n",
    "batch_size = 100        # size of the minibatch\n",
    "precision = 1e-30       # parameter to control numerical precision of weight updates\n",
    "anti_hebbian_learning_strength = 0.4    # Strength of the anti-hebbian learning\n",
    "lebesgue_norm = 2.0                     # Lebesgue norm of the weights\n",
    "rank = 2                                # ranking parameter, must be integer that is bigger or equal than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED 'BIO' LEARNING ALGORITHM\n",
    "\n",
    "# Define function that performs the unsupervised learning and returns weights\n",
    "# that correspond to feature detectors.\n",
    "# Uses cuda if available.\n",
    "def get_unsupervised_weights(data, n_hidden, n_epochs, batch_size, learning_rate, precision, \n",
    "                             anti_hebbian_learning_strength, lebesgue_norm, rank):\n",
    "    print(\"Starting unsupervised bio-plausible training\")\n",
    "    \n",
    "    num_samples = data.shape[0]   # Number of samples/images.\n",
    "    num_features = data.shape[1]  # Number of pixels for each sample/image.\n",
    "    \n",
    "    # Initialize weights to be values drawn from gaussian distribution.\n",
    "    synapses = np.random.normal(mu, sigma, (n_hidden, num_features)).astype(np.float32)\n",
    "    weights = torch.from_numpy(synapses).to(device)\n",
    "\n",
    "    # The external loop runs over epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        eps = learning_rate * (1 - epoch / n_epochs)\n",
    "        #print(f'epoch learning rate: {eps}')\n",
    "\n",
    "        # Scramble the images and values. So that when making a\n",
    "        # mini batch, random values/images will be chosen on each iteration.\n",
    "        random_permutation_samples = np.random.permutation(num_samples)\n",
    "        shuffled_epoch_data = data[random_permutation_samples,:]\n",
    "\n",
    "        # Internal loop runs over minibatches    \n",
    "        for i in range(num_samples // batch_size):        \n",
    "            # For every minibatch the overlap with the data (tot_input) is \n",
    "            # calculated for each data point and each hidden unit.\n",
    "            mini_batch = shuffled_epoch_data[i*batch_size:(i+1)*batch_size,:].astype(np.float32)\n",
    "            mini_batch = torch.from_numpy(mini_batch).to(device)           \n",
    "            mini_batch = torch.transpose(mini_batch, 0, 1)\n",
    "\n",
    "            sign = torch.sign(weights)            \n",
    "            W = sign * torch.abs(weights) ** (lebesgue_norm - 1)    \n",
    "            # https://stackoverflow.com/questions/44524901/how-to-do-product-of-matrices-in-pytorch\n",
    "            tot_input_torch = torch.mm(W, mini_batch)\n",
    "\n",
    "            # The sorted strengths of the activations are stored in y. \n",
    "            # The variable yl stores the activations of the post synaptic cells - \n",
    "            # it is denoted by g(Q) in Eq 3 of 'Unsupervised Learning by Competing Hidden Units', see also Eq 9 and Eq 10.        \n",
    "            y_torch = torch.argsort(tot_input_torch, dim=0)            \n",
    "            yl_torch = torch.zeros((n_hidden, batch_size), dtype = torch.float).to(device)\n",
    "            yl_torch[y_torch[n_hidden-1,:], torch.arange(batch_size)] = 1.0\n",
    "            yl_torch[y_torch[n_hidden-rank], torch.arange(batch_size)] = -anti_hebbian_learning_strength\n",
    "\n",
    "            # The variable ds is the right hand side of Eq 3        \n",
    "            xx_torch = torch.sum(yl_torch * tot_input_torch,1)  \n",
    "            xx_torch = xx_torch.unsqueeze(1)                    \n",
    "            xx_torch = xx_torch.repeat(1, num_features)\n",
    "            ds_torch = torch.mm(yl_torch, torch.transpose(mini_batch, 0, 1)) - (xx_torch * weights)\n",
    "\n",
    "            # Update weights\n",
    "            # The weights are updated after each minibatch in a way so that the largest update \n",
    "            # is equal to the learning rate eps at that epoch.        \n",
    "            nc_torch = torch.max(torch.abs(ds_torch))\n",
    "            if nc_torch < precision: \n",
    "                nc_torch = precision            \n",
    "            weights += eps*(ds_torch/nc_torch)\n",
    "\n",
    "            #if (i+1) % 100 == 0:\n",
    "            #    print (f'Epoch [{epoch+1}/{n_epochs}], Step [{i+1}/{num_samples // batch_size}]')\n",
    "\n",
    "        print (f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "        \n",
    "    print(\"Completed unsupervised bio-plausible training\")\n",
    "    return weights.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD AND PREPARE DATA\n",
    "\n",
    "print(\"Loading MNIST...\")\n",
    "mat = scipy.io.loadmat('mnist_all.mat')\n",
    "print(\"Done loading MNIST\")\n",
    "\n",
    "Nc=10 # number of classes\n",
    "N=784 # number of pixels for each image. 28x28\n",
    "\n",
    "M=np.zeros((0,N))\n",
    "for i in range(Nc):\n",
    "    M=np.concatenate((M, mat['train'+str(i)]), axis=0)\n",
    "M=M/255.0\n",
    "\n",
    "data = M\n",
    "print(f'Number of samples: {data.shape[0]}')\n",
    "print(f'Number of features: {data.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RUN UNSUPERVISED 'BIO' LEARNING ALGORITHM\n",
    "\n",
    "# Calculates weights for data and provided number of hidden units (given other configuration)\n",
    "weights = get_unsupervised_weights(data, n_hidden, n_epochs, batch_size, learning_rate, precision, \n",
    "                                   anti_hebbian_learning_strength, lebesgue_norm, rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw weights/feature detectors generated by unsupervised bio algo\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "fig=plt.figure(figsize=(12.9,10))\n",
    "draw_weights(weights, Kx, Ky)\n",
    "print(\"Fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS BLOCK FOR DEBUGGING PURPOSES ONLY\n",
    "\n",
    "# Contains data loading and whole bio learning in one block of code.\n",
    "# Plots the feature detectors at the end of training.\n",
    "\n",
    "# LOAD AND PREPARE DATA\n",
    "\n",
    "print(\"Loading MNIST...\")\n",
    "mat = scipy.io.loadmat('mnist_all.mat')\n",
    "print(\"Done loading MNIST\")\n",
    "\n",
    "Nc=10 # output nodes\n",
    "N=784 # number of pixels for each image. 28x28\n",
    "\n",
    "M=np.zeros((0,N))\n",
    "for i in range(Nc):\n",
    "    M=np.concatenate((M, mat['train'+str(i)]), axis=0)\n",
    "M=M/255.0\n",
    "\n",
    "data = M\n",
    "num_samples = data.shape[0]   # 60000 training and validation examples. Number of samples\n",
    "num_features = data.shape[1]  # number of pixels for each image. 28x28. Also: num_samples, num_pixels..\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# UNSUPERVISED 'BIO' LEARNING ALGORITHM\n",
    "\n",
    "# Initialize weights to be values drawn from gaussian distribution.\n",
    "synapses = np.random.normal(mu, sigma, (n_hidden, N)).astype(np.float32)\n",
    "weights = torch.from_numpy(synapses).to(device)\n",
    "\n",
    "# The external loop runs over epochs\n",
    "for epoch in range(n_epochs):\n",
    "    eps = learning_rate * (1 - epoch / n_epochs)\n",
    "    #print(f'epoch learning rate: {eps}')\n",
    "    \n",
    "    # Scramble the images and values. So that when making a\n",
    "    # mini batch, random values/images will be chosen on each iteration.\n",
    "    random_permutation_samples = np.random.permutation(num_samples)\n",
    "    shuffled_epoch_data = data[random_permutation_samples,:]\n",
    "    \n",
    "    # Internal loop runs over minibatches    \n",
    "    for i in range(num_samples // batch_size):        \n",
    "        # For every minibatch the overlap with the data (tot_input) is \n",
    "        # calculated for each data point and each hidden unit.\n",
    "        mini_batch = shuffled_epoch_data[i*batch_size:(i+1)*batch_size,:].astype(np.float32)\n",
    "        mini_batch = torch.from_numpy(mini_batch).to(device)           \n",
    "        mini_batch = torch.transpose(mini_batch, 0, 1)\n",
    "        \n",
    "        sign = torch.sign(weights)            \n",
    "        W = sign * torch.abs(weights) ** (lebesgue_norm - 1)    \n",
    "        # https://stackoverflow.com/questions/44524901/how-to-do-product-of-matrices-in-pytorch\n",
    "        tot_input_torch = torch.mm(W, mini_batch)\n",
    "                \n",
    "        # The sorted strengths of the activations are stored in y. \n",
    "        # The variable yl stores the activations of the post synaptic cells - \n",
    "        # it is denoted by g(Q) in Eq 3 of 'Unsupervised Learning by Competing Hidden Units', see also Eq 9 and Eq 10.        \n",
    "        y_torch = torch.argsort(tot_input_torch, dim=0)            \n",
    "        yl_torch = torch.zeros((n_hidden, batch_size), dtype = torch.float).to(device)\n",
    "        yl_torch[y_torch[n_hidden-1,:], torch.arange(batch_size)] = 1.0\n",
    "        yl_torch[y_torch[n_hidden-rank], torch.arange(batch_size)] = -anti_hebbian_learning_strength\n",
    "        \n",
    "        # The variable ds is the right hand side of Eq 3        \n",
    "        xx_torch = torch.sum(yl_torch * tot_input_torch,1)  \n",
    "        xx_torch = xx_torch.unsqueeze(1)                    \n",
    "        xx_torch = xx_torch.repeat(1, num_features)\n",
    "        ds_torch = torch.mm(yl_torch, torch.transpose(mini_batch, 0, 1)) - (xx_torch * weights)\n",
    "        \n",
    "        # Update weights\n",
    "        # The weights are updated after each minibatch in a way so that the largest update \n",
    "        # is equal to the learning rate eps at that epoch.        \n",
    "        nc_torch = torch.max(torch.abs(ds_torch))\n",
    "        if nc_torch < precision: \n",
    "            nc_torch = precision            \n",
    "        weights += eps*(ds_torch/nc_torch)\n",
    "        \n",
    "        #if (i+1) % 100 == 0:\n",
    "        #    print (f'Epoch [{epoch+1}/{n_epochs}], Step [{i+1}/{num_samples // batch_size}]')\n",
    "        \n",
    "    print (f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "    #draw_weights(weights.numpy(), Kx, Ky)\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "fig=plt.figure(figsize=(12.9,10))\n",
    "draw_weights(weights.cpu().numpy(), Kx, Ky)\n",
    "print(\"Fin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
