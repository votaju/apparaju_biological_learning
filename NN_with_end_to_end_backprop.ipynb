{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sourced from: https://www.python-engineer.com/courses/pytorchbeginner/13-feedforward-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 500 \n",
    "num_classes = 10\n",
    "num_epochs = 200\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# TODO adjust these as per the configuration in the paper.\n",
    "# Especially how learning rate changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPICAL PIPELINE\n",
    "\n",
    "# 1) LOAD DATA\n",
    "# 2) DESIGN MODEL\n",
    "# 3) LOSS AND OPTIMIZER\n",
    "# 4) TRAINING LOOP\n",
    "#        - FORWARD PASS: computer prediction and calculate error\n",
    "#        - BACKWARD PASS: calculate gradients\n",
    "#        - UPDATE WEIGHTS\n",
    "#        - SET ZERO GRAD IF USING AUTOGRAD\n",
    "\n",
    "# 5) TEST MODEL WITH TEST DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) LOAD DATA\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "#examples = iter(test_loader)\n",
    "#example_data, example_targets = examples.next()\n",
    "\n",
    "#for i in range(9):\n",
    "#    plt.subplot(3,3,i+1)\n",
    "#    plt.imshow(example_data[i][0], cmap='gray')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) DESIGN MODEL\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'params': [Parameter containing:\n",
      "tensor([[-0.0087,  0.0287,  0.0297,  ...,  0.0263,  0.0336,  0.0272],\n",
      "        [-0.0288,  0.0178, -0.0305,  ...,  0.0109,  0.0225,  0.0176],\n",
      "        [-0.0059, -0.0055,  0.0299,  ..., -0.0139,  0.0298,  0.0131],\n",
      "        ...,\n",
      "        [ 0.0247,  0.0092, -0.0346,  ..., -0.0223, -0.0058, -0.0100],\n",
      "        [-0.0284, -0.0182,  0.0313,  ...,  0.0176, -0.0336, -0.0087],\n",
      "        [-0.0003, -0.0265,  0.0338,  ..., -0.0208,  0.0354,  0.0350]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-1.0755e-02, -8.3936e-03,  3.0748e-02,  3.0286e-02, -3.0292e-02,\n",
      "         3.0231e-02, -3.1381e-02,  2.7698e-02, -9.6403e-03, -1.5125e-02,\n",
      "         9.9192e-03, -2.1362e-02,  4.4716e-03,  3.5070e-02, -2.3002e-02,\n",
      "         1.7972e-03,  3.3925e-02,  2.8830e-02, -3.5374e-02, -3.0989e-02,\n",
      "        -3.0631e-02,  8.6100e-03, -2.4656e-02, -2.5020e-02, -2.1210e-02,\n",
      "         3.4999e-02,  5.3167e-03,  2.7774e-03, -1.7985e-02,  2.1601e-02,\n",
      "         3.5485e-02,  2.7138e-02, -2.7482e-02, -1.9999e-03,  2.0472e-02,\n",
      "         2.5619e-03,  7.2148e-03, -2.4052e-02, -9.8202e-03, -6.9241e-03,\n",
      "         1.8345e-02, -2.8184e-02,  2.2124e-02, -2.2208e-03,  2.1370e-02,\n",
      "         1.8298e-02,  6.4361e-03,  2.2331e-02,  7.1353e-03,  2.2246e-02,\n",
      "        -2.1947e-02,  2.5384e-02,  9.1033e-03,  8.9615e-03,  3.1072e-02,\n",
      "        -2.5390e-03,  9.9988e-03,  8.3173e-03,  3.9804e-03,  1.9036e-02,\n",
      "        -3.0296e-02, -9.9047e-03,  3.2447e-02, -2.0254e-02,  7.6296e-03,\n",
      "         1.8075e-02, -3.3535e-02,  2.6636e-02,  2.7370e-03, -1.1740e-03,\n",
      "         3.3871e-02,  3.1829e-02, -5.6933e-03,  2.7903e-02,  1.5591e-02,\n",
      "         7.1835e-03, -5.7115e-04, -2.5588e-02, -1.3189e-02, -6.5903e-03,\n",
      "        -2.7347e-02,  3.1731e-02,  4.7974e-03,  2.6122e-02, -2.5893e-04,\n",
      "         3.5683e-02,  1.6002e-02, -1.6132e-02, -7.5535e-03, -2.3063e-02,\n",
      "         7.0597e-03,  6.0708e-03, -2.7336e-02, -2.1373e-02, -3.0360e-02,\n",
      "        -3.3989e-02,  2.6812e-02, -2.8676e-02, -3.1442e-02,  9.8632e-03,\n",
      "        -3.1715e-02,  1.9315e-02,  1.3272e-02, -2.0828e-03,  7.3122e-03,\n",
      "         2.4808e-02, -2.3786e-02, -9.1742e-03,  3.0607e-02,  5.4259e-03,\n",
      "         1.9671e-02, -1.8224e-02, -1.9087e-02, -3.1763e-02, -1.5861e-02,\n",
      "        -1.8263e-02, -7.9306e-03,  1.2159e-02, -5.0776e-03, -7.5136e-03,\n",
      "         2.3319e-02,  2.6390e-02, -3.4853e-03,  2.9498e-02, -2.7956e-02,\n",
      "        -1.2589e-02,  3.7165e-04,  1.0899e-02,  3.3787e-02, -8.0669e-03,\n",
      "        -2.1256e-02, -2.1964e-02,  2.5127e-02, -2.5464e-02, -3.0399e-02,\n",
      "         2.4192e-02, -3.1887e-03,  1.7379e-02, -1.4105e-02,  2.5221e-02,\n",
      "        -2.2260e-02,  4.3789e-03,  2.5186e-02,  2.5362e-02,  2.3520e-02,\n",
      "        -2.5565e-02, -2.9037e-02,  3.2345e-02, -1.7869e-02,  2.3598e-02,\n",
      "         1.7254e-02,  3.8680e-03,  1.9470e-02,  2.2128e-02,  1.6299e-02,\n",
      "         2.1618e-02, -2.4589e-02,  2.4694e-02, -2.6864e-02,  7.9569e-04,\n",
      "         2.6633e-02, -2.2913e-02,  3.5533e-02,  8.5349e-03, -1.0902e-02,\n",
      "        -2.8172e-02,  1.9494e-02, -8.8876e-03, -1.0916e-02,  1.1874e-02,\n",
      "        -2.2550e-02,  2.0686e-02,  1.9051e-02,  3.0380e-02,  4.2210e-03,\n",
      "         3.1055e-02,  1.0389e-02,  1.7433e-02,  1.4815e-02,  1.8433e-02,\n",
      "        -9.1412e-03,  1.4397e-02, -1.8174e-02, -2.1591e-02,  1.9913e-02,\n",
      "         4.7131e-03,  2.2112e-02, -9.4108e-03, -2.6330e-02, -3.0662e-02,\n",
      "        -1.0238e-02,  2.3654e-02, -8.1622e-03,  2.0484e-02,  2.2412e-02,\n",
      "        -1.1276e-02, -2.2653e-02,  1.2756e-02,  2.6855e-02, -1.1067e-02,\n",
      "         1.1055e-02, -1.1971e-03, -2.5931e-02, -2.5676e-02, -1.3235e-02,\n",
      "        -2.9857e-02,  1.4281e-02,  3.5505e-02,  2.2130e-03,  3.1297e-02,\n",
      "         3.1710e-02, -5.8541e-03,  2.8646e-02, -1.8009e-02, -1.2850e-02,\n",
      "        -8.3274e-03,  4.5815e-03, -5.6129e-03, -2.5175e-02, -6.4282e-03,\n",
      "         2.8528e-03,  2.2378e-02,  2.1218e-02, -3.5551e-02,  3.5472e-02,\n",
      "         4.0508e-03,  6.1511e-03,  2.1121e-02, -1.9326e-02,  1.8403e-02,\n",
      "         2.4290e-02, -1.4317e-02, -1.0774e-04,  1.7929e-02,  1.0064e-02,\n",
      "         7.5261e-03, -3.1430e-02, -3.1126e-02, -2.4360e-02, -6.1695e-03,\n",
      "        -1.3774e-03,  8.3335e-04,  1.7411e-02,  1.6504e-02, -1.6165e-02,\n",
      "        -1.3645e-02, -2.9857e-02,  3.1322e-03, -3.3529e-02,  1.2003e-02,\n",
      "        -5.2600e-03,  1.8547e-02,  3.1639e-02, -3.2095e-02,  2.3404e-02,\n",
      "         1.7640e-02,  2.2308e-02,  2.0837e-02,  6.8064e-03,  2.4842e-02,\n",
      "        -3.2737e-03, -2.0865e-02, -1.0720e-02,  7.6216e-03,  2.6672e-02,\n",
      "        -2.3570e-02,  1.3199e-02, -3.4020e-02,  7.0609e-03,  3.5306e-03,\n",
      "        -2.3104e-02, -3.2980e-02,  3.0094e-02,  2.5859e-02,  5.6667e-03,\n",
      "        -1.2502e-02, -1.9203e-03, -2.1648e-02,  6.6426e-04, -1.1756e-02,\n",
      "         2.0917e-02, -7.3628e-03, -5.5403e-03,  1.8550e-02, -1.5287e-02,\n",
      "         8.2240e-03, -3.3211e-02, -9.6241e-03, -3.1221e-02, -8.8743e-03,\n",
      "        -1.9260e-02, -2.9475e-02,  1.1787e-02, -1.4951e-02,  1.5158e-02,\n",
      "        -1.5457e-02, -3.3536e-03,  3.4145e-02, -2.9970e-02, -1.4925e-02,\n",
      "         9.3637e-03, -3.1566e-03, -2.5105e-04, -2.5472e-02,  1.1749e-02,\n",
      "         1.5449e-02,  4.0457e-03, -1.6793e-02,  2.5216e-02, -2.7840e-03,\n",
      "        -2.4840e-02, -1.3457e-03, -5.9541e-03,  1.7788e-02, -9.2345e-03,\n",
      "        -1.6554e-02,  3.6139e-04, -6.3112e-03, -1.9721e-02,  1.1364e-02,\n",
      "         2.4313e-02,  1.0401e-02,  2.6165e-02,  3.0028e-02, -6.1511e-03,\n",
      "         1.1320e-02,  1.9899e-02,  1.7144e-02, -3.5546e-03, -9.9941e-03,\n",
      "         2.3941e-02,  2.1645e-02, -3.4485e-02, -1.1165e-02,  1.8870e-02,\n",
      "         2.1445e-02, -2.6496e-02, -2.4011e-02,  1.6145e-02, -3.3606e-02,\n",
      "         2.7396e-02, -1.5000e-02,  2.9337e-03,  3.3337e-02, -1.0587e-02,\n",
      "         3.0198e-02, -2.9567e-02, -3.4632e-02, -1.7699e-02, -1.2115e-02,\n",
      "         2.6399e-02, -2.8627e-03, -1.4743e-02, -1.1647e-02, -5.9713e-03,\n",
      "        -9.3794e-03,  3.0478e-02, -1.6725e-03, -2.4983e-02, -1.6301e-02,\n",
      "        -1.2342e-02, -1.8141e-02, -1.7360e-03,  3.7373e-03,  9.0410e-03,\n",
      "         1.7797e-02, -1.8125e-02, -1.0903e-02,  1.7583e-02, -2.1847e-02,\n",
      "         2.5216e-02, -1.1254e-02,  9.6125e-03, -2.1971e-02, -3.2524e-02,\n",
      "         3.4251e-02, -3.2597e-02,  2.5357e-02, -1.3485e-02, -2.8315e-02,\n",
      "        -3.4163e-02, -3.1614e-02, -2.8640e-02, -4.1620e-03, -3.3927e-02,\n",
      "        -6.1942e-03, -2.4295e-02,  2.0172e-02, -3.7645e-03,  4.2468e-03,\n",
      "         3.0317e-02, -4.8038e-03,  1.6620e-02, -1.5242e-02,  2.2186e-02,\n",
      "        -5.6380e-03, -3.2846e-02,  2.2110e-02,  1.3316e-02,  7.6342e-03,\n",
      "        -1.3245e-02,  2.9645e-02, -2.9425e-02, -3.3964e-02,  1.8857e-02,\n",
      "         1.2253e-02, -2.4768e-02,  2.1284e-02, -1.7808e-02,  1.3459e-03,\n",
      "         1.9421e-02, -2.4594e-02,  3.4110e-03, -1.5575e-02, -4.1364e-03,\n",
      "        -5.2101e-03, -3.0380e-02, -1.8210e-02, -3.2808e-02,  1.3735e-02,\n",
      "        -2.3791e-02, -1.8550e-03,  2.3968e-02,  2.0650e-02,  1.8572e-03,\n",
      "         7.0133e-03, -3.3350e-02, -1.8742e-02, -1.7047e-02,  5.8159e-03,\n",
      "         4.2936e-03, -1.7459e-02, -5.7157e-03, -8.9356e-03, -3.5174e-02,\n",
      "         1.9066e-03,  3.4204e-03, -3.5020e-02,  2.4158e-03,  1.5859e-02,\n",
      "        -1.3823e-02, -5.6855e-03, -2.9248e-02,  4.3201e-03, -1.5646e-02,\n",
      "         2.8176e-02,  1.3851e-02, -2.1345e-02, -3.5370e-02, -3.4504e-02,\n",
      "        -1.8904e-02,  5.2106e-03,  1.4819e-02,  3.5203e-02,  4.5613e-03,\n",
      "        -7.6863e-03,  1.5901e-02, -2.7394e-02, -4.3948e-03, -2.8057e-02,\n",
      "        -1.6455e-02, -1.0189e-02,  6.3787e-03, -3.0912e-02,  1.9937e-03,\n",
      "         9.8484e-04,  1.2983e-02,  3.4321e-02,  7.5660e-03, -1.5304e-02,\n",
      "         2.5349e-02, -1.8244e-02,  2.5409e-02,  2.1650e-02, -2.6811e-02,\n",
      "         3.4856e-03, -1.0958e-02, -3.3853e-02, -1.7686e-02,  3.2905e-02,\n",
      "        -2.7910e-02, -2.9935e-02,  2.4167e-02,  2.9342e-02, -1.8832e-02,\n",
      "         1.8645e-05, -3.5373e-02, -1.0800e-02, -2.2316e-02, -1.6666e-02,\n",
      "        -6.0232e-03, -7.9981e-03,  2.6758e-02,  2.6078e-02, -3.0086e-02,\n",
      "         3.0671e-03, -2.3675e-02,  1.6242e-02,  3.2998e-02, -2.6142e-02],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0026,  0.0126,  0.0205,  ...,  0.0178, -0.0037,  0.0077],\n",
      "        [ 0.0102, -0.0337, -0.0260,  ..., -0.0306,  0.0068,  0.0093],\n",
      "        [-0.0434,  0.0042, -0.0328,  ...,  0.0406,  0.0099, -0.0122],\n",
      "        ...,\n",
      "        [ 0.0368,  0.0164,  0.0185,  ...,  0.0115,  0.0003, -0.0079],\n",
      "        [-0.0226, -0.0419, -0.0188,  ..., -0.0399,  0.0279,  0.0071],\n",
      "        [ 0.0415, -0.0247, -0.0016,  ...,  0.0020,  0.0273, -0.0029]],\n",
      "       device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.0112, -0.0280,  0.0087, -0.0011,  0.0208, -0.0341, -0.0349,  0.0059,\n",
      "         0.0101,  0.0368], device='cuda:0', requires_grad=True)], 'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}]\n",
      "param_group[lr]: 0.001\n"
     ]
    }
   ],
   "source": [
    "# 3) LOSS AND OPTIMIZER\n",
    "\n",
    "# TODO: Use the custom loss function in th paper. Equation [12]. It seems to use one-hot encoded labels.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "print(optimizer.param_groups)\n",
    "\n",
    "# TODO: To dynamically change learning rate\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"param_group[lr]:\", param_group['lr'])\n",
    "#    param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "60000 600\n",
      "Epoch [1/200], Step [100/600], Loss: 0.2730\n",
      "Epoch [1/200], Step [200/600], Loss: 0.2155\n",
      "Epoch [1/200], Step [300/600], Loss: 0.1878\n",
      "Epoch [1/200], Step [400/600], Loss: 0.0769\n",
      "Epoch [1/200], Step [500/600], Loss: 0.1746\n",
      "Epoch [1/200], Step [600/600], Loss: 0.1515\n",
      "Epoch [2/200], Step [100/600], Loss: 0.0499\n",
      "Epoch [2/200], Step [200/600], Loss: 0.1281\n",
      "Epoch [2/200], Step [300/600], Loss: 0.0625\n",
      "Epoch [2/200], Step [400/600], Loss: 0.0453\n",
      "Epoch [2/200], Step [500/600], Loss: 0.1371\n",
      "Epoch [2/200], Step [600/600], Loss: 0.1116\n",
      "Epoch [3/200], Step [100/600], Loss: 0.0315\n",
      "Epoch [3/200], Step [200/600], Loss: 0.0809\n",
      "Epoch [3/200], Step [300/600], Loss: 0.0237\n",
      "Epoch [3/200], Step [400/600], Loss: 0.0370\n",
      "Epoch [3/200], Step [500/600], Loss: 0.0587\n",
      "Epoch [3/200], Step [600/600], Loss: 0.1005\n",
      "Epoch [4/200], Step [100/600], Loss: 0.0919\n",
      "Epoch [4/200], Step [200/600], Loss: 0.0882\n",
      "Epoch [4/200], Step [300/600], Loss: 0.0214\n",
      "Epoch [4/200], Step [400/600], Loss: 0.0561\n",
      "Epoch [4/200], Step [500/600], Loss: 0.0315\n",
      "Epoch [4/200], Step [600/600], Loss: 0.0551\n",
      "Epoch [5/200], Step [100/600], Loss: 0.0204\n",
      "Epoch [5/200], Step [200/600], Loss: 0.0340\n",
      "Epoch [5/200], Step [300/600], Loss: 0.0180\n",
      "Epoch [5/200], Step [400/600], Loss: 0.0574\n",
      "Epoch [5/200], Step [500/600], Loss: 0.0432\n",
      "Epoch [5/200], Step [600/600], Loss: 0.0557\n",
      "Epoch [6/200], Step [100/600], Loss: 0.0103\n",
      "Epoch [6/200], Step [200/600], Loss: 0.0100\n",
      "Epoch [6/200], Step [300/600], Loss: 0.0892\n",
      "Epoch [6/200], Step [400/600], Loss: 0.0161\n",
      "Epoch [6/200], Step [500/600], Loss: 0.0350\n",
      "Epoch [6/200], Step [600/600], Loss: 0.0311\n",
      "Epoch [7/200], Step [100/600], Loss: 0.0075\n",
      "Epoch [7/200], Step [200/600], Loss: 0.0260\n",
      "Epoch [7/200], Step [300/600], Loss: 0.0186\n",
      "Epoch [7/200], Step [400/600], Loss: 0.0256\n",
      "Epoch [7/200], Step [500/600], Loss: 0.0108\n",
      "Epoch [7/200], Step [600/600], Loss: 0.0067\n",
      "Epoch [8/200], Step [100/600], Loss: 0.0151\n",
      "Epoch [8/200], Step [200/600], Loss: 0.0209\n",
      "Epoch [8/200], Step [300/600], Loss: 0.0279\n",
      "Epoch [8/200], Step [400/600], Loss: 0.0034\n",
      "Epoch [8/200], Step [500/600], Loss: 0.0119\n",
      "Epoch [8/200], Step [600/600], Loss: 0.0022\n",
      "Epoch [9/200], Step [100/600], Loss: 0.0062\n",
      "Epoch [9/200], Step [200/600], Loss: 0.0010\n",
      "Epoch [9/200], Step [300/600], Loss: 0.0042\n",
      "Epoch [9/200], Step [400/600], Loss: 0.0159\n",
      "Epoch [9/200], Step [500/600], Loss: 0.0323\n",
      "Epoch [9/200], Step [600/600], Loss: 0.0014\n",
      "Epoch [10/200], Step [100/600], Loss: 0.0070\n",
      "Epoch [10/200], Step [200/600], Loss: 0.0090\n",
      "Epoch [10/200], Step [300/600], Loss: 0.0163\n",
      "Epoch [10/200], Step [400/600], Loss: 0.0083\n",
      "Epoch [10/200], Step [500/600], Loss: 0.0063\n",
      "Epoch [10/200], Step [600/600], Loss: 0.0100\n",
      "Epoch [11/200], Step [100/600], Loss: 0.0027\n",
      "Epoch [11/200], Step [200/600], Loss: 0.0095\n",
      "Epoch [11/200], Step [300/600], Loss: 0.0069\n",
      "Epoch [11/200], Step [400/600], Loss: 0.0049\n",
      "Epoch [11/200], Step [500/600], Loss: 0.0055\n",
      "Epoch [11/200], Step [600/600], Loss: 0.0016\n",
      "Epoch [12/200], Step [100/600], Loss: 0.0057\n",
      "Epoch [12/200], Step [200/600], Loss: 0.0424\n",
      "Epoch [12/200], Step [300/600], Loss: 0.0045\n",
      "Epoch [12/200], Step [400/600], Loss: 0.0018\n",
      "Epoch [12/200], Step [500/600], Loss: 0.0051\n",
      "Epoch [12/200], Step [600/600], Loss: 0.0132\n",
      "Epoch [13/200], Step [100/600], Loss: 0.0205\n",
      "Epoch [13/200], Step [200/600], Loss: 0.0054\n",
      "Epoch [13/200], Step [300/600], Loss: 0.0060\n",
      "Epoch [13/200], Step [400/600], Loss: 0.0031\n",
      "Epoch [13/200], Step [500/600], Loss: 0.0039\n",
      "Epoch [13/200], Step [600/600], Loss: 0.0176\n",
      "Epoch [14/200], Step [100/600], Loss: 0.0037\n",
      "Epoch [14/200], Step [200/600], Loss: 0.0011\n",
      "Epoch [14/200], Step [300/600], Loss: 0.0048\n",
      "Epoch [14/200], Step [400/600], Loss: 0.0019\n",
      "Epoch [14/200], Step [500/600], Loss: 0.0019\n",
      "Epoch [14/200], Step [600/600], Loss: 0.0244\n",
      "Epoch [15/200], Step [100/600], Loss: 0.0029\n",
      "Epoch [15/200], Step [200/600], Loss: 0.0042\n",
      "Epoch [15/200], Step [300/600], Loss: 0.0027\n",
      "Epoch [15/200], Step [400/600], Loss: 0.0043\n",
      "Epoch [15/200], Step [500/600], Loss: 0.0004\n",
      "Epoch [15/200], Step [600/600], Loss: 0.0013\n",
      "Epoch [16/200], Step [100/600], Loss: 0.0029\n",
      "Epoch [16/200], Step [200/600], Loss: 0.0035\n",
      "Epoch [16/200], Step [300/600], Loss: 0.0018\n",
      "Epoch [16/200], Step [400/600], Loss: 0.0031\n",
      "Epoch [16/200], Step [500/600], Loss: 0.0011\n",
      "Epoch [16/200], Step [600/600], Loss: 0.0005\n",
      "Epoch [17/200], Step [100/600], Loss: 0.0004\n",
      "Epoch [17/200], Step [200/600], Loss: 0.0065\n",
      "Epoch [17/200], Step [300/600], Loss: 0.0055\n",
      "Epoch [17/200], Step [400/600], Loss: 0.0073\n",
      "Epoch [17/200], Step [500/600], Loss: 0.0058\n",
      "Epoch [17/200], Step [600/600], Loss: 0.0021\n",
      "Epoch [18/200], Step [100/600], Loss: 0.0008\n",
      "Epoch [18/200], Step [200/600], Loss: 0.0013\n",
      "Epoch [18/200], Step [300/600], Loss: 0.0016\n",
      "Epoch [18/200], Step [400/600], Loss: 0.0043\n",
      "Epoch [18/200], Step [500/600], Loss: 0.0007\n",
      "Epoch [18/200], Step [600/600], Loss: 0.0148\n",
      "Epoch [19/200], Step [100/600], Loss: 0.0034\n",
      "Epoch [19/200], Step [200/600], Loss: 0.0005\n",
      "Epoch [19/200], Step [300/600], Loss: 0.0011\n",
      "Epoch [19/200], Step [400/600], Loss: 0.0013\n",
      "Epoch [19/200], Step [500/600], Loss: 0.0006\n",
      "Epoch [19/200], Step [600/600], Loss: 0.0005\n",
      "Epoch [20/200], Step [100/600], Loss: 0.0009\n",
      "Epoch [20/200], Step [200/600], Loss: 0.0015\n",
      "Epoch [20/200], Step [300/600], Loss: 0.0210\n",
      "Epoch [20/200], Step [400/600], Loss: 0.0002\n",
      "Epoch [20/200], Step [500/600], Loss: 0.0011\n",
      "Epoch [20/200], Step [600/600], Loss: 0.0005\n",
      "Epoch [21/200], Step [100/600], Loss: 0.0003\n",
      "Epoch [21/200], Step [200/600], Loss: 0.0002\n",
      "Epoch [21/200], Step [300/600], Loss: 0.0029\n",
      "Epoch [21/200], Step [400/600], Loss: 0.0034\n",
      "Epoch [21/200], Step [500/600], Loss: 0.0003\n",
      "Epoch [21/200], Step [600/600], Loss: 0.0025\n",
      "Epoch [22/200], Step [100/600], Loss: 0.0042\n",
      "Epoch [22/200], Step [200/600], Loss: 0.0202\n",
      "Epoch [22/200], Step [300/600], Loss: 0.0005\n",
      "Epoch [22/200], Step [400/600], Loss: 0.0017\n",
      "Epoch [22/200], Step [500/600], Loss: 0.0023\n",
      "Epoch [22/200], Step [600/600], Loss: 0.0008\n",
      "Epoch [23/200], Step [100/600], Loss: 0.0020\n",
      "Epoch [23/200], Step [200/600], Loss: 0.0001\n",
      "Epoch [23/200], Step [300/600], Loss: 0.0002\n",
      "Epoch [23/200], Step [400/600], Loss: 0.0003\n",
      "Epoch [23/200], Step [500/600], Loss: 0.0005\n",
      "Epoch [23/200], Step [600/600], Loss: 0.0013\n",
      "Epoch [24/200], Step [100/600], Loss: 0.0003\n",
      "Epoch [24/200], Step [200/600], Loss: 0.0004\n",
      "Epoch [24/200], Step [300/600], Loss: 0.0003\n",
      "Epoch [24/200], Step [400/600], Loss: 0.0001\n",
      "Epoch [24/200], Step [500/600], Loss: 0.0001\n",
      "Epoch [24/200], Step [600/600], Loss: 0.0005\n",
      "Epoch [25/200], Step [100/600], Loss: 0.0001\n",
      "Epoch [25/200], Step [200/600], Loss: 0.0011\n",
      "Epoch [25/200], Step [300/600], Loss: 0.0031\n",
      "Epoch [25/200], Step [400/600], Loss: 0.0030\n",
      "Epoch [25/200], Step [500/600], Loss: 0.0227\n",
      "Epoch [25/200], Step [600/600], Loss: 0.0020\n",
      "Epoch [26/200], Step [100/600], Loss: 0.0019\n",
      "Epoch [26/200], Step [200/600], Loss: 0.0001\n",
      "Epoch [26/200], Step [300/600], Loss: 0.0002\n",
      "Epoch [26/200], Step [400/600], Loss: 0.0002\n",
      "Epoch [26/200], Step [500/600], Loss: 0.0013\n",
      "Epoch [26/200], Step [600/600], Loss: 0.0020\n",
      "Epoch [27/200], Step [100/600], Loss: 0.0001\n",
      "Epoch [27/200], Step [200/600], Loss: 0.0010\n",
      "Epoch [27/200], Step [300/600], Loss: 0.0001\n",
      "Epoch [27/200], Step [400/600], Loss: 0.0001\n",
      "Epoch [27/200], Step [500/600], Loss: 0.0001\n",
      "Epoch [27/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [28/200], Step [100/600], Loss: 0.0002\n",
      "Epoch [28/200], Step [200/600], Loss: 0.0001\n",
      "Epoch [28/200], Step [300/600], Loss: 0.0001\n",
      "Epoch [28/200], Step [400/600], Loss: 0.0003\n",
      "Epoch [28/200], Step [500/600], Loss: 0.0001\n",
      "Epoch [28/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [29/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [29/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [29/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [29/200], Step [400/600], Loss: 0.0001\n",
      "Epoch [29/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [29/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [30/200], Step [100/600], Loss: 0.0001\n",
      "Epoch [30/200], Step [200/600], Loss: 0.0001\n",
      "Epoch [30/200], Step [300/600], Loss: 0.0003\n",
      "Epoch [30/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [30/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [30/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [31/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [31/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [31/200], Step [300/600], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [31/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [31/200], Step [600/600], Loss: 0.0001\n",
      "Epoch [32/200], Step [100/600], Loss: 0.0001\n",
      "Epoch [32/200], Step [200/600], Loss: 0.0001\n",
      "Epoch [32/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [32/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [32/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [32/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [33/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [33/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [33/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [33/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [33/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [33/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [34/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [34/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [34/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [34/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [34/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [34/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [35/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [35/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [35/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [35/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [35/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [35/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [36/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [36/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [36/200], Step [300/600], Loss: 0.0199\n",
      "Epoch [36/200], Step [400/600], Loss: 0.0176\n",
      "Epoch [36/200], Step [500/600], Loss: 0.0024\n",
      "Epoch [36/200], Step [600/600], Loss: 0.0015\n",
      "Epoch [37/200], Step [100/600], Loss: 0.0009\n",
      "Epoch [37/200], Step [200/600], Loss: 0.0008\n",
      "Epoch [37/200], Step [300/600], Loss: 0.0002\n",
      "Epoch [37/200], Step [400/600], Loss: 0.0002\n",
      "Epoch [37/200], Step [500/600], Loss: 0.0009\n",
      "Epoch [37/200], Step [600/600], Loss: 0.0002\n",
      "Epoch [38/200], Step [100/600], Loss: 0.0001\n",
      "Epoch [38/200], Step [200/600], Loss: 0.0002\n",
      "Epoch [38/200], Step [300/600], Loss: 0.0196\n",
      "Epoch [38/200], Step [400/600], Loss: 0.0013\n",
      "Epoch [38/200], Step [500/600], Loss: 0.0001\n",
      "Epoch [38/200], Step [600/600], Loss: 0.0002\n",
      "Epoch [39/200], Step [100/600], Loss: 0.0001\n",
      "Epoch [39/200], Step [200/600], Loss: 0.0001\n",
      "Epoch [39/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [39/200], Step [400/600], Loss: 0.0001\n",
      "Epoch [39/200], Step [500/600], Loss: 0.0010\n",
      "Epoch [39/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [40/200], Step [100/600], Loss: 0.0001\n",
      "Epoch [40/200], Step [200/600], Loss: 0.0001\n",
      "Epoch [40/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [40/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [40/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [40/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [41/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [41/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [41/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [41/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [41/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [41/200], Step [600/600], Loss: 0.0001\n",
      "Epoch [42/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [42/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [42/200], Step [300/600], Loss: 0.0001\n",
      "Epoch [42/200], Step [400/600], Loss: 0.0001\n",
      "Epoch [42/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [42/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [43/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [43/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [43/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [43/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [43/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [43/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [44/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [44/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [44/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [44/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [44/200], Step [500/600], Loss: 0.0001\n",
      "Epoch [44/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [45/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [45/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [45/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [45/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [45/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [45/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [46/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [46/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [46/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [46/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [46/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [46/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [47/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [47/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [47/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [47/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [47/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [47/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [48/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [48/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [48/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [48/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [48/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [48/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [49/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [49/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [49/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [49/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [49/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [49/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [50/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [50/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [50/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [50/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [50/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [50/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [51/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [51/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [51/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [51/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [51/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [51/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [52/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [52/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [52/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [52/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [52/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [52/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [53/200], Step [100/600], Loss: 0.0161\n",
      "Epoch [53/200], Step [200/600], Loss: 0.0003\n",
      "Epoch [53/200], Step [300/600], Loss: 0.0018\n",
      "Epoch [53/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [53/200], Step [500/600], Loss: 0.0087\n",
      "Epoch [53/200], Step [600/600], Loss: 0.0002\n",
      "Epoch [54/200], Step [100/600], Loss: 0.0002\n",
      "Epoch [54/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [54/200], Step [300/600], Loss: 0.0005\n",
      "Epoch [54/200], Step [400/600], Loss: 0.0004\n",
      "Epoch [54/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [54/200], Step [600/600], Loss: 0.0004\n",
      "Epoch [55/200], Step [100/600], Loss: 0.0001\n",
      "Epoch [55/200], Step [200/600], Loss: 0.0001\n",
      "Epoch [55/200], Step [300/600], Loss: 0.0001\n",
      "Epoch [55/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [55/200], Step [500/600], Loss: 0.0001\n",
      "Epoch [55/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [56/200], Step [100/600], Loss: 0.0002\n",
      "Epoch [56/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [56/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [56/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [56/200], Step [500/600], Loss: 0.0007\n",
      "Epoch [56/200], Step [600/600], Loss: 0.0001\n",
      "Epoch [57/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [57/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [57/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [57/200], Step [400/600], Loss: 0.0002\n",
      "Epoch [57/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [57/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [58/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [58/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [58/200], Step [300/600], Loss: 0.0001\n",
      "Epoch [58/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [58/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [58/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [59/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [59/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [59/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [59/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [59/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [59/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [60/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [60/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [60/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [60/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [60/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [60/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [61/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [61/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [61/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [61/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [61/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [61/200], Step [600/600], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [62/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [62/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [62/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [62/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [62/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [63/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [63/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [63/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [63/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [63/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [63/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [64/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [64/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [64/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [64/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [64/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [64/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [65/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [65/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [65/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [65/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [65/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [65/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [66/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [66/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [66/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [66/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [66/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [66/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [67/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [67/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [67/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [67/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [67/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [67/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [68/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [68/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [68/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [68/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [68/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [68/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [69/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [69/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [69/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [69/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [69/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [69/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [70/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [70/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [70/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [70/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [70/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [70/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [71/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [71/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [71/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [71/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [71/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [71/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [72/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [72/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [72/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [72/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [72/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [72/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [73/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [73/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [73/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [73/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [73/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [73/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [74/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [74/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [74/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [74/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [74/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [74/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [75/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [75/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [75/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [75/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [75/200], Step [500/600], Loss: 0.0471\n",
      "Epoch [75/200], Step [600/600], Loss: 0.0015\n",
      "Epoch [76/200], Step [100/600], Loss: 0.0017\n",
      "Epoch [76/200], Step [200/600], Loss: 0.0094\n",
      "Epoch [76/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [76/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [76/200], Step [500/600], Loss: 0.0007\n",
      "Epoch [76/200], Step [600/600], Loss: 0.0400\n",
      "Epoch [77/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [77/200], Step [200/600], Loss: 0.0025\n",
      "Epoch [77/200], Step [300/600], Loss: 0.0009\n",
      "Epoch [77/200], Step [400/600], Loss: 0.0005\n",
      "Epoch [77/200], Step [500/600], Loss: 0.0003\n",
      "Epoch [77/200], Step [600/600], Loss: 0.0001\n",
      "Epoch [78/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [78/200], Step [200/600], Loss: 0.0005\n",
      "Epoch [78/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [78/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [78/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [78/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [79/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [79/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [79/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [79/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [79/200], Step [500/600], Loss: 0.0003\n",
      "Epoch [79/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [80/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [80/200], Step [200/600], Loss: 0.0001\n",
      "Epoch [80/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [80/200], Step [400/600], Loss: 0.0007\n",
      "Epoch [80/200], Step [500/600], Loss: 0.0001\n",
      "Epoch [80/200], Step [600/600], Loss: 0.0002\n",
      "Epoch [81/200], Step [100/600], Loss: 0.0177\n",
      "Epoch [81/200], Step [200/600], Loss: 0.0026\n",
      "Epoch [81/200], Step [300/600], Loss: 0.0003\n",
      "Epoch [81/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [81/200], Step [500/600], Loss: 0.0005\n",
      "Epoch [81/200], Step [600/600], Loss: 0.0002\n",
      "Epoch [82/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [82/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [82/200], Step [300/600], Loss: 0.0038\n",
      "Epoch [82/200], Step [400/600], Loss: 0.0001\n",
      "Epoch [82/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [82/200], Step [600/600], Loss: 0.0001\n",
      "Epoch [83/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [83/200], Step [200/600], Loss: 0.0004\n",
      "Epoch [83/200], Step [300/600], Loss: 0.0001\n",
      "Epoch [83/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [83/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [83/200], Step [600/600], Loss: 0.0001\n",
      "Epoch [84/200], Step [100/600], Loss: 0.0723\n",
      "Epoch [84/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [84/200], Step [300/600], Loss: 0.0027\n",
      "Epoch [84/200], Step [400/600], Loss: 0.0023\n",
      "Epoch [84/200], Step [500/600], Loss: 0.0001\n",
      "Epoch [84/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [85/200], Step [100/600], Loss: 0.0001\n",
      "Epoch [85/200], Step [200/600], Loss: 0.0009\n",
      "Epoch [85/200], Step [300/600], Loss: 0.0001\n",
      "Epoch [85/200], Step [400/600], Loss: 0.0013\n",
      "Epoch [85/200], Step [500/600], Loss: 0.0002\n",
      "Epoch [85/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [86/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [86/200], Step [200/600], Loss: 0.0002\n",
      "Epoch [86/200], Step [300/600], Loss: 0.0001\n",
      "Epoch [86/200], Step [400/600], Loss: 0.0001\n",
      "Epoch [86/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [86/200], Step [600/600], Loss: 0.0001\n",
      "Epoch [87/200], Step [100/600], Loss: 0.0002\n",
      "Epoch [87/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [87/200], Step [300/600], Loss: 0.0001\n",
      "Epoch [87/200], Step [400/600], Loss: 0.0151\n",
      "Epoch [87/200], Step [500/600], Loss: 0.0001\n",
      "Epoch [87/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [88/200], Step [100/600], Loss: 0.0001\n",
      "Epoch [88/200], Step [200/600], Loss: 0.0004\n",
      "Epoch [88/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [88/200], Step [400/600], Loss: 0.0031\n",
      "Epoch [88/200], Step [500/600], Loss: 0.0001\n",
      "Epoch [88/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [89/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [89/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [89/200], Step [300/600], Loss: 0.0067\n",
      "Epoch [89/200], Step [400/600], Loss: 0.0341\n",
      "Epoch [89/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [89/200], Step [600/600], Loss: 0.0024\n",
      "Epoch [90/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [90/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [90/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [90/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [90/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [90/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [91/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [91/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [91/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [91/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [91/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [91/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [92/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [92/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [92/200], Step [300/600], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [92/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [92/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [93/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [93/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [93/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [93/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [93/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [93/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [94/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [94/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [94/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [94/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [94/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [94/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [95/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [95/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [95/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [95/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [95/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [95/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [96/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [96/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [96/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [96/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [96/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [96/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [97/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [97/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [97/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [97/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [97/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [97/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [98/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [98/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [98/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [98/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [98/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [98/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [99/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [99/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [99/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [99/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [99/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [99/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [100/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [100/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [100/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [100/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [100/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [100/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [101/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [101/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [101/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [101/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [101/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [101/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [102/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [102/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [102/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [102/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [102/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [102/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [103/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [103/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [103/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [103/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [103/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [103/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [104/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [104/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [104/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [104/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [104/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [104/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [105/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [105/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [105/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [105/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [105/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [105/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [106/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [106/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [106/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [106/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [106/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [106/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [107/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [107/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [107/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [107/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [107/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [107/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [108/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [108/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [108/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [108/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [108/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [108/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [109/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [109/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [109/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [109/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [109/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [109/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [110/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [110/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [110/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [110/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [110/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [110/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [111/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [111/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [111/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [111/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [111/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [111/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [112/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [112/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [112/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [112/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [112/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [112/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [113/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [113/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [113/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [113/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [113/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [113/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [114/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [114/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [114/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [114/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [114/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [114/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [115/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [115/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [115/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [115/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [115/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [115/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [116/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [116/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [116/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [116/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [116/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [116/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [117/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [117/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [117/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [117/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [117/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [117/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [118/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [118/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [118/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [118/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [118/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [118/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [119/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [119/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [119/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [119/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [119/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [119/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [120/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [120/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [120/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [120/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [120/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [120/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [121/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [121/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [121/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [121/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [121/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [121/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [122/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [122/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [122/200], Step [300/600], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [122/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [122/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [122/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [123/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [123/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [123/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [123/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [123/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [123/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [124/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [124/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [124/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [124/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [124/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [124/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [125/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [125/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [125/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [125/200], Step [400/600], Loss: 0.0313\n",
      "Epoch [125/200], Step [500/600], Loss: 0.0012\n",
      "Epoch [125/200], Step [600/600], Loss: 0.0480\n",
      "Epoch [126/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [126/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [126/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [126/200], Step [400/600], Loss: 0.0001\n",
      "Epoch [126/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [126/200], Step [600/600], Loss: 0.0071\n",
      "Epoch [127/200], Step [100/600], Loss: 0.0034\n",
      "Epoch [127/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [127/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [127/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [127/200], Step [500/600], Loss: 0.0004\n",
      "Epoch [127/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [128/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [128/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [128/200], Step [300/600], Loss: 0.0311\n",
      "Epoch [128/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [128/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [128/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [129/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [129/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [129/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [129/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [129/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [129/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [130/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [130/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [130/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [130/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [130/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [130/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [131/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [131/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [131/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [131/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [131/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [131/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [132/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [132/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [132/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [132/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [132/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [132/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [133/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [133/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [133/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [133/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [133/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [133/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [134/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [134/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [134/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [134/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [134/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [134/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [135/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [135/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [135/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [135/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [135/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [135/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [136/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [136/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [136/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [136/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [136/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [136/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [137/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [137/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [137/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [137/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [137/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [137/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [138/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [138/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [138/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [138/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [138/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [138/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [139/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [139/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [139/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [139/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [139/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [139/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [140/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [140/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [140/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [140/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [140/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [140/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [141/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [141/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [141/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [141/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [141/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [141/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [142/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [142/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [142/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [142/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [142/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [142/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [143/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [143/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [143/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [143/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [143/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [143/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [144/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [144/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [144/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [144/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [144/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [144/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [145/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [145/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [145/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [145/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [145/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [145/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [146/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [146/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [146/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [146/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [146/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [146/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [147/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [147/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [147/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [147/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [147/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [147/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [148/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [148/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [148/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [148/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [148/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [148/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [149/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [149/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [149/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [149/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [149/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [149/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [150/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [150/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [150/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [150/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [150/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [150/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [151/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [151/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [151/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [151/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [151/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [151/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [152/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [152/200], Step [200/600], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [152/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [152/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [152/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [152/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [153/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [153/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [153/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [153/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [153/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [153/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [154/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [154/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [154/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [154/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [154/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [154/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [155/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [155/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [155/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [155/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [155/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [155/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [156/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [156/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [156/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [156/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [156/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [156/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [157/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [157/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [157/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [157/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [157/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [157/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [158/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [158/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [158/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [158/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [158/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [158/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [159/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [159/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [159/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [159/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [159/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [159/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [160/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [160/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [160/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [160/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [160/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [160/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [161/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [161/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [161/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [161/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [161/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [161/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [162/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [162/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [162/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [162/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [162/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [162/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [163/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [163/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [163/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [163/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [163/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [163/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [164/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [164/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [164/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [164/200], Step [400/600], Loss: 0.0989\n",
      "Epoch [164/200], Step [500/600], Loss: 0.0322\n",
      "Epoch [164/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [165/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [165/200], Step [200/600], Loss: 0.0003\n",
      "Epoch [165/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [165/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [165/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [165/200], Step [600/600], Loss: 0.0012\n",
      "Epoch [166/200], Step [100/600], Loss: 0.0004\n",
      "Epoch [166/200], Step [200/600], Loss: 0.0002\n",
      "Epoch [166/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [166/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [166/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [166/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [167/200], Step [100/600], Loss: 0.0208\n",
      "Epoch [167/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [167/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [167/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [167/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [167/200], Step [600/600], Loss: 0.0001\n",
      "Epoch [168/200], Step [100/600], Loss: 0.0012\n",
      "Epoch [168/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [168/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [168/200], Step [400/600], Loss: 0.0001\n",
      "Epoch [168/200], Step [500/600], Loss: 0.0021\n",
      "Epoch [168/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [169/200], Step [100/600], Loss: 0.0011\n",
      "Epoch [169/200], Step [200/600], Loss: 0.0010\n",
      "Epoch [169/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [169/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [169/200], Step [500/600], Loss: 0.0018\n",
      "Epoch [169/200], Step [600/600], Loss: 0.0002\n",
      "Epoch [170/200], Step [100/600], Loss: 0.0185\n",
      "Epoch [170/200], Step [200/600], Loss: 0.0007\n",
      "Epoch [170/200], Step [300/600], Loss: 0.0001\n",
      "Epoch [170/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [170/200], Step [500/600], Loss: 0.0299\n",
      "Epoch [170/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [171/200], Step [100/600], Loss: 0.0068\n",
      "Epoch [171/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [171/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [171/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [171/200], Step [500/600], Loss: 0.0003\n",
      "Epoch [171/200], Step [600/600], Loss: 0.0002\n",
      "Epoch [172/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [172/200], Step [200/600], Loss: 0.0008\n",
      "Epoch [172/200], Step [300/600], Loss: 0.0031\n",
      "Epoch [172/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [172/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [172/200], Step [600/600], Loss: 0.0003\n",
      "Epoch [173/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [173/200], Step [200/600], Loss: 0.0251\n",
      "Epoch [173/200], Step [300/600], Loss: 0.0008\n",
      "Epoch [173/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [173/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [173/200], Step [600/600], Loss: 0.0003\n",
      "Epoch [174/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [174/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [174/200], Step [300/600], Loss: 0.0001\n",
      "Epoch [174/200], Step [400/600], Loss: 0.0245\n",
      "Epoch [174/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [174/200], Step [600/600], Loss: 0.0017\n",
      "Epoch [175/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [175/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [175/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [175/200], Step [400/600], Loss: 0.0230\n",
      "Epoch [175/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [175/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [176/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [176/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [176/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [176/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [176/200], Step [500/600], Loss: 0.0001\n",
      "Epoch [176/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [177/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [177/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [177/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [177/200], Step [400/600], Loss: 0.0001\n",
      "Epoch [177/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [177/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [178/200], Step [100/600], Loss: 0.0001\n",
      "Epoch [178/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [178/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [178/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [178/200], Step [500/600], Loss: 0.0003\n",
      "Epoch [178/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [179/200], Step [100/600], Loss: 0.0001\n",
      "Epoch [179/200], Step [200/600], Loss: 0.0415\n",
      "Epoch [179/200], Step [300/600], Loss: 0.0007\n",
      "Epoch [179/200], Step [400/600], Loss: 0.0015\n",
      "Epoch [179/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [179/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [180/200], Step [100/600], Loss: 0.0002\n",
      "Epoch [180/200], Step [200/600], Loss: 0.0004\n",
      "Epoch [180/200], Step [300/600], Loss: 0.0148\n",
      "Epoch [180/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [180/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [180/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [181/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [181/200], Step [200/600], Loss: 0.0025\n",
      "Epoch [181/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [181/200], Step [400/600], Loss: 0.0001\n",
      "Epoch [181/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [181/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [182/200], Step [100/600], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [182/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [182/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [182/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [182/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [182/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [183/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [183/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [183/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [183/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [183/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [183/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [184/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [184/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [184/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [184/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [184/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [184/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [185/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [185/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [185/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [185/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [185/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [185/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [186/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [186/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [186/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [186/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [186/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [186/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [187/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [187/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [187/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [187/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [187/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [187/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [188/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [188/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [188/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [188/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [188/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [188/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [189/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [189/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [189/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [189/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [189/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [189/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [190/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [190/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [190/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [190/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [190/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [190/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [191/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [191/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [191/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [191/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [191/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [191/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [192/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [192/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [192/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [192/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [192/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [192/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [193/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [193/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [193/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [193/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [193/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [193/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [194/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [194/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [194/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [194/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [194/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [194/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [195/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [195/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [195/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [195/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [195/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [195/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [196/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [196/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [196/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [196/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [196/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [196/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [197/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [197/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [197/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [197/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [197/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [197/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [198/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [198/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [198/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [198/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [198/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [198/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [199/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [199/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [199/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [199/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [199/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [199/200], Step [600/600], Loss: 0.0000\n",
      "Epoch [200/200], Step [100/600], Loss: 0.0000\n",
      "Epoch [200/200], Step [200/600], Loss: 0.0000\n",
      "Epoch [200/200], Step [300/600], Loss: 0.0000\n",
      "Epoch [200/200], Step [400/600], Loss: 0.0000\n",
      "Epoch [200/200], Step [500/600], Loss: 0.0000\n",
      "Epoch [200/200], Step [600/600], Loss: 0.0000\n",
      "Accuracy of the network on the 10000 test images: 98.33 %\n"
     ]
    }
   ],
   "source": [
    "# 4) TRAINING LOOP\n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "print(n_total_steps)\n",
    "\n",
    "total_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "'''\n",
    "# Dummy training loop for printing \n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        \n",
    "        # here: 60000 samples, batch_size = 100, n_iters=60000/100 = 600 iterations\n",
    "        # Run your training process\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{n_iterations}| Inputs {images.shape} | Labels {labels.shape}')\n",
    "'''\n",
    "\n",
    "# Real training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # original shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        # original shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOG\n",
    "\n",
    "for param in model.parameters():\n",
    "  print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
