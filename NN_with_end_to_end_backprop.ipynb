{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sourced from: https://www.python-engineer.com/courses/pytorchbeginner/13-feedforward-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784 # 28x28 for MNIST\n",
    "input_size_mnist = 784 # 28x28 for MNIST\n",
    "input_size_cifar_gray = 1024 # 1x32x32 for CIFAR-10 grayscale\n",
    "input_size_cifar_rgb = 3072 # 3x32x32 for CIFAR-10 rgb\n",
    "hidden_size = 2000 \n",
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.004\n",
    "\n",
    "# TODO adjust these as per the configuration in the paper.\n",
    "# Especially how learning rate changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPICAL PIPELINE\n",
    "\n",
    "# 1) LOAD DATA\n",
    "# 2) DESIGN MODEL\n",
    "# 3) LOSS AND OPTIMIZER\n",
    "# 4) TRAINING LOOP\n",
    "#        - FORWARD PASS: computer prediction and calculate error\n",
    "#        - BACKWARD PASS: calculate gradients\n",
    "#        - UPDATE WEIGHTS\n",
    "#        - SET ZERO GRAD IF USING AUTOGRAD\n",
    "\n",
    "# 5) TEST MODEL WITH TEST DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) LOAD MNIST DATA\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 1) LOAD CIFAR-10 DATA\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "transform_grayscale = transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.ToTensor()])\n",
    "\n",
    "# CIFAR-10 dataset \n",
    "# train_dataset = torchvision.datasets.CIFAR10(root='./gray_data', \n",
    "#                                             train=True, \n",
    "#                                             transform=transform,  \n",
    "#                                             download=True)\n",
    "\n",
    "# test_dataset = torchvision.datasets.CIFAR10(root='./gray_data', \n",
    "#                                           train=False, \n",
    "#                                           transform=transform)\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                            train=True, \n",
    "                                            transform=transform,  \n",
    "                                            download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 32, 32)\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAe7ElEQVR4nO2daYyc13Wm31NfLb1vbLLZXEVJlBVZiSmF1tiJRpGdcaAoCWQDgccewFAAIwqCCIiBzA/BA4w9wPxwBmMb/jHwgB5rrBgeyxrbgoREyNiWgwiGHUnURi3UQnGRSDbZJJu9d+1nflTJQ2nue7vJZlfTvu8DEKy+p+/3nbr1nfqq71vnHHN3CCF+/cmttwNCiM6gYBciERTsQiSCgl2IRFCwC5EICnYhEiG/mslmdgeArwHIAPwPd/9S7Pf7u/O+YaAYPlb8PBftW0xSdHBb9FxkWvR4/Ghxo8feh2P+h20WOxmZAwAxZfbSZFvuR+xo7hd/DbSOydaD04w+6UvzI/bsmKUZcYP5OLNQx1KlEXTykoPdzDIA/w3AxwAcB/C0mT3q7q+wORsGivjCv7s+fDxv0nMVC2E3LccDolqtUFu9UePnKobfjACg0Qz76JFXxXINastl1ASv9fJjgh+zUCwHx7PIS2057n+jWae2Wp2/Zs0mCQrjftTD1ygAoMKOh+UCN+xj7E29WuXXR6MRWcfINZyLvGZVcl0t8KXHYjV8vG//5ETEh0vnFgCH3P2wu1cBPAjgrlUcTwixhqwm2LcCePuCn4+3x4QQVyBrvkFnZveY2X4z2z+/FPlcIoRYU1YT7CcAbL/g523tsXfh7vvcfa+77+3rXtV+oBBiFawm2J8GsNvMdplZEcCnADx6edwSQlxuLvlW6+51M7sXwP9BS3q7391fjs6BoUreX9yX+ESyW1kC37HOgW915/ORHfJLULyswCdVqlVqqzcjPkaktyyyi58n06zJd5hR58pFbBe5GfG/al3B8UZW4nNix2vw9bAm99GImtAVec3yxm25fES5qEXW2PifsE7W2CM6Q5aFfYwpE6v6XO3ujwF4bDXHEEJ0Bn2DTohEULALkQgKdiESQcEuRCIo2IVIhA5/y8XhLLHCufzjjfAca3CpplnjklfWHZFxwJMZmOTVjEg/xUKB2urObc1a5LlFzlevh20WyeTKRWQ+y3hikGdheQ0Alhphie3UOS5PLVS5j/PzfF7mfD36u8LrWDT+Og/0dFNbd4lLaM0cv+ZyURkt7CO/OoAaS76KaG+6swuRCAp2IRJBwS5EIijYhUgEBbsQidDR3XhzR75Bdt2zyG4xSeIoZZH8+HxsWzKS6EASDADQRJh6rFhYjvtRKPJd381XXUdts9Nnqe3sucXwufJ8Vz2HSHJKnV8iS879P3gs7KOXRuicWsYTm6p9fOd/fmaK2k5MTgfH+0r8eTVOhecAwI4xvo4b+vk6duVj5azC13Excgk3iAIRK7elO7sQiaBgFyIRFOxCJIKCXYhEULALkQgKdiESYR3KvYalAcsP8RlETqjHOnDkuCxXrfOEhWKkRlqjQWqFRRJTEJFCipE6aP/q33yM2p75+S+o7eT0ueD4QkRCqze45HXs+BlqO3KCdx8pDY0Hx7eN7aJzvNRPbdU8f10KfRuprV6eD46fmzxJ5/QMcXnw+PxpaiuTWokAMNbP01p6CuFEmEYtLKMCAGviE+nkpTu7EKmgYBciERTsQiSCgl2IRFCwC5EICnYhEmFV0puZHQUwB6ABoO7ue2O/37QcKrmwvDKz2EPnNUh7ouE+Lq8NZFwOy0fqsTUjshyTNWhdPcSz6BYXz1PbT//+EWo7Pc3r9Z2eD5/v2Al+rmMTb1Nb1tVHbY1sgNp6B0aD44Uefrx8F8+iK0VaMnXluHR4thpuKza+bQedU15aoLYjR7j0NjVTprbM+PO+amPYVmhwKc9YXcaI1Hs5dPaPuDvPuRRCXBHoY7wQibDaYHcAPzKzZ8zsnsvhkBBibVjtx/hb3f2EmW0C8GMze9Xdn7jwF9pvAvcAwHA/r/IhhFhbVnVnd/cT7f8nATwM4JbA7+xz973uvrevex2+ii+EALCKYDezXjPrf+cxgD8A8NLlckwIcXlZza12DMDD7a3+PID/5e7/GJtQbxrOLIUzfKZqPOvtiZ//c3D8N3ZzyeUj7w9LPwAwHClu2SSZbQCQI216cjme0dRw3rYooibhyLEj1Da1xDPAvGc4OJ71ceknNzxHbd1Dg9RWLXOpqUraKw0M89dsoI/bJk+dorbZ87zgZH8xfIl3dXOZ763zXFwq9G+itjOn3qK2vtN8jTcPhH3ptkimIinCioisfMnB7u6HAXzgUucLITqLpDchEkHBLkQiKNiFSAQFuxCJoGAXIhE62+stKyE/GC44uHiOv+/UiuGCglOLYSkMABarvDfYQJFntjVJ3622MTicZTxjr1zlEs8ZnryGs3NcAowVRBzeGM7mWmjO0jmj4D5mkUy0aoGvY3khLDWV57kfO8c2UNsikdAAYJJktgGAFcIy5cwUL+aISAHRpQWeEZcV+XUwOcuzDidIttzOUX5951hCXKzFITcJIX6dULALkQgKdiESQcEuRCIo2IVIhI7uxnd19+J9v/X/ZcECAI7/y2t0Xt9geDf+lg+HjwUAPdkxaquSnWIAyOV5UosVwjvTDedJPP2btlPb8wcOUVvfEN+Z3rrz/dTmufDucyGyc96shFtGAUC1GmmxFVmrjCRxvPzCATpnoBRpkdTLk2R6I3XtTp4K14yrE2UFADKygw8Aw/1cnZhp8KSn81PcduTUTHB8y9hmOifPFKVIdpXu7EIkgoJdiERQsAuRCAp2IRJBwS5EIijYhUiEjkpvuSyPnsGwpLTz6uvovCWiWuzYdS2dM1rj0sr0ES7L1SKJMI16ONHhlts+TufsuJp3xNr1m0ep7ZnnXqC24T4uyZycDNdPyzsv410qcMkLfBkxH0kKmSF14YZ7+bkip0IjIpWNbgxLswBQqYVfz7Pnw3IXAFikZVd/pE5ePuPhVC3zxJvDbx8Pjm8c4jLf7m3hNmoeuX/rzi5EIijYhUgEBbsQiaBgFyIRFOxCJIKCXYhEWFZ6M7P7AfwxgEl3v7E9NgLgewCuAnAUwCfdnRfZeudYuRyyUjhD6eTpg3Tent/+YHC8d5DX/MrmTlBbox5pkROpdXb47XC23K3D4bp6AICebdTU38vlmK48z+TqjtQ66yqSjK1IXbWtW8ap7ZU336S2YpHX+ZudC6/VVdt20znXXX8DtU1N8curb4BnHZ48NRkctxyv7zY0zGv8zURqyWURya67h/u4NBe+Dg6R6w0Auovhc9XqkSxFavl/fAvAHe8Zuw/A4+6+G8Dj7Z+FEFcwywZ7u9/6e78hcReAB9qPHwDAv1UihLgiuNS/2cfcfaL9+BRaHV2FEFcwq96gc3dH5JuOZnaPme03s/0zM7xmuBBibbnUYD9tZuMA0P4/vAsCwN33ufted987ODhwiacTQqyWSw32RwHc3X58N4BHLo87Qoi1YiXS23cB3A5g1MyOA/gCgC8BeMjMPgvgGIBPruRkZhkKXeG7e7nMCyJWKuG0t0JEgurp5Z8ieiMtjUoZz3rry4f7NX1r3zfpnD/5t/dSW2HhFLUVS5HspRz3cdfVW4Pjk1Mn6ZzyPM9e27xplNqmZrl0WKmGX8+rr+WZitdcyzMfZ557ltoW5uapbXYh7GO9wSWqpaVwOyYAGBoapLaGc6lsYIhn+9Wr4dczy/H+YMcnwh+mqyTLD1hBsLv7p4np95ebK4S4ctA36IRIBAW7EImgYBciERTsQiSCgl2IROhowUmYwbKwBLEYkX/Ki0vB8UKkJ9fcOZ7lhYxLbwXwQoTjQ+FMqTcO8p5tJ49zGxa5HHbs+FFqu2kz73G3dWe4GOWWSf6N5oVDvADnSCnSx26Iy3KHDx8Njo9vCUuDADA9y79hWYtIZafP8F51TbfguEWKQy5GpDfL8esqfKYWvZFClWiGs+yKFr7uAaB6LizbeqRsp+7sQiSCgl2IRFCwC5EICnYhEkHBLkQiKNiFSITOSm8OgPTsypxLK+Oj4f5wPV1cevvpAV4ocThSlG/3CM9O6iqFZZdinks1ZyaPUluzwosX7riGF7HMIs+7Z2A4OD46xgtfnpviWWMzkcy2RkTd3Ej6r+UjcmmZZH8B8WyupTLPDqsTJ9k4AJQrPAOzXuf3xw2jm6jNjF9XRQtfPyWL9B30cMZnIVL0Und2IRJBwS5EIijYhUgEBbsQiaBgFyIROrobbwYU8uFkksE+npwy1B+2WZPvVs46Tzw4e56nLIz28yXpLYZ3VBu5cI08ADh68ii1jQ3zemY7r+WtkMr8dHjqmXAbrRMTfOe/vy+8gw8AhQJv8fTyobe4I+Q+0ozcXyqR3fj5BZ4UMjTC2zXVSSLMxGlaEBm9/fx1yWc80aSnh9dELLK2XABQCyfyNBam6ZSxTf3B8XyBt7XSnV2IRFCwC5EICnYhEkHBLkQiKNiFSAQFuxCJsJL2T/cD+GMAk+5+Y3vsiwD+HMCZ9q993t0fW8kJMwtLIZs3hWuntZwkMk4kAWJ8G08k2R+Rw6aNS3aehevkDY7ypIrBAZ4AUegKyycAcFVEeusbDCcGAcD/vP/bwfHFyFrNLk1R2+ISrw1YiFw9m4fDz7s8xevdLZBEIwAYHOCvy6uvvUFtp0+fCY7PRlpGDQ3xJzbQ20dtmXNNtFDl65iRWoQbe/nxBrvCcZSP3L5Xcmf/FoA7AuNfdfc97X8rCnQhxPqxbLC7+xMA+Fu/EOJXgtX8zX6vmR0ws/vNjH8FSwhxRXCpwf51ANcA2ANgAsCX2S+a2T1mtt/M9k9P86//CSHWlksKdnc/7e4Nd28C+AYA2rXA3fe5+1533zs0xBsOCCHWlksKdjMbv+DHTwB46fK4I4RYK1YivX0XwO0ARs3sOIAvALjdzPagVVXuKIC/WMnJcrkczf4ZGObSW70RdrOU55lE1+3aQW37n+GS12zhWmpr2lxwfGwrl9deOfgv1PY7v/dn1PaLn/N5CwuRNknVs8HxyVNv0zmx9/z5GrflwaWh4Vw4y25rN/d95gyX0OoZ3xYa28RtjUY4k24p0uKpvMTr7i1EaujVm1zOq5VPUNumQjijb0sfz6Kr1MNzYnfvZYPd3T8dGP7mcvOEEFcW+gadEImgYBciERTsQiSCgl2IRFCwC5EIHS04mcvl0NsXzl4aHh2l8+oWdrOcK9I5XX0D1DY0xAsKvvX2KWq79YPvD/sxz9tJ9fSHs64AYOLEcWo79Prr1FZv8PZEOVJvcGF2hs7p3zBObTMzXIYa7OPFKN933Y3B8adfeJXOefbVo9R26+1/SG2FIpeoDh86FByfmePPK1YUs7zE5bWdY1zS7e7lBVVHRsLzPM8LcNar4cKXTrJKAd3ZhUgGBbsQiaBgFyIRFOxCJIKCXYhEULALkQgdld7cm2jWw5LH4Agv5LewFC5EuNjgfbeyjL+P7di+jdpef5lnXs0shiW2vl6eYbf9GmrCsdd58cUTJyeo7cMf/iC1LS6GpaH+LVvpnJEtvDjnW1NcKluqcMmx2BvuvzawcTudc1M/f13OnAn3QwOAo8deoLaFpbBMOT3DJbSNGzdS26Dz12VnH5dENw3wHmwFC2cCVmu8v10vkdhy4DGhO7sQiaBgFyIRFOxCJIKCXYhEULALkQgd3Y1v1muYOxfezeyO1PaqlMO7nNbk7pvxXcnREd4+6fXcYWqbnAq38DmX8V3pwT5eW+/6G3lCzuFjvGZcjXdJwvRsWO3YvXs3nbN7F5cMjk3wBJqXX36R2s6dDSenFEtcdRnu44kkx1/mqsCpc7yunZFkqSzSeivWOmwnzzPBjn6eGNSV40ktlXL4+mk2eW3DWp0cj1/2urMLkQoKdiESQcEuRCIo2IVIBAW7EImgYBciEVbS/mk7gL8DMIbWxv4+d/+amY0A+B6Aq9BqAfVJdw/3/GlTqVRw+FBY2tqx+zfovK5cWHprVnmiQL4rIoNEbP39XBrqGwjXtbv++vfROT/50WPUtjjD6931jGyitkPHJ6lt+7ZwUs6u991M55SK/DK4egdP8pme4i/3KwfDCUVN57rhiWmeSDJLkqEAoNzgsu3sdFiK3LSZJ928dY7XpxvZzuXScyXuB5r8uU3Xw8/N8/w6rZDjVcETblZyZ68D+Bt3vwHAhwD8lZndAOA+AI+7+24Aj7d/FkJcoSwb7O4+4e7Pth/PATgIYCuAuwA80P61BwB8fK2cFEKsnov6m93MrgJwE4AnAYy5/zK59xRaH/OFEFcoKw52M+sD8AMAn3P3d30/0d0d5It6ZnaPme03s/1zc7xggBBibVlRsJtZAa1A/467/7A9fNrMxtv2cQDBXSN33+fue919b2zzSwixtiwb7GZmaPVjP+juX7nA9CiAu9uP7wbwyOV3TwhxuVhJ1tvvAvgMgBfN7Pn22OcBfAnAQ2b2WQDHAHxyuQMtVup4/lBYNtpx4y10XhPhbDNjmT8A0OTpP7Nzc9Q2PX2W2jaM7AmO33nHR+icPR+4ntoe+uHD1GbGJZTBwWFq27olLCn1DQzROVk9vL4AMLKZXyLju2rUNtMdlo2ee4HXi5uY5yllXuDtvAY38yzG0WvCUlkWkbUazv14zcPtywDg0CkuDxYzfsylcjk4vhi5vOvN8PUx1+DZgcsGu7v/DADz9PeXmy+EuDLQN+iESAQFuxCJoGAXIhEU7EIkgoJdiEToaMHJcsPw+kx30Ha2wQsAeiEsTeSqvBiiE2kCAHI5btsyzrPN/vXvhDPHugpcctm1k7dd+qM//RS1ff/hf6C2s6f4856YCRcvLJcP0TlFcI1naonbDh3jWXuohmU5H+UZgsObwkUqAaAZqaTY+s4XmdcVPmbTwoUoAaAWaSs20+Dn6irwY3blufS2YOEsu1qBn8ub4fVtRCRb3dmFSAQFuxCJoGAXIhEU7EIkgoJdiERQsAuRCB2V3ioNw+vT4feXR37G+4bt2TkaHN9c5BlIPYVIttZm3n9tfJRnV11zNSlS6LyY4MSZc9R2/4NcXnv2+VeojfW+AwCaCOj8fd0b/HiNEl+PRo5LQ3mEJdZ6RBqq58JzAKArdqVGstTK1fDz9hyfk49kxGVN3tfPy1ymrIPPKzTDPmbGX7NqLex/pMWh7uxCpIKCXYhEULALkQgKdiESQcEuRCJ0dDe+AcN8Lpws8Pizr9N5b7wZbhl1x2/fQOdcs4W36TlyONyaCABu++CN1NZFEhPmqnyH+aF/fJrannvlJLUt1iOthCK7xblC+P27GanJlzO+ixzbtW40eQJQheww1xp8jhmvaVdBJCnE+XPL58lOd8bvcz09PKGlCO5/g2+4o2E81BpkYr3GX5dif7imoOX4eXRnFyIRFOxCJIKCXYhEULALkQgKdiESQcEuRCIsK72Z2XYAf4dWS2YHsM/dv2ZmXwTw5wDOtH/18+7+WPRk+Tw2jG4M2qbOc/lk4vx0cPznL/BWN43azognXFrZuJkkuwCwLCyHPbX/JTrnH376C2qrNHnNNeS59JbLXfx7dKPCk108Iss1I/JaTPJiLZQKeX7JWcYlTGT8NctH5mVZ+HyxJqNZZH1zzuXBRiTZqBmRDplmt3kzl4/7B8K2N0uRdeIe/JI6gL9x92fNrB/AM2b247btq+7+X1dwDCHEOrOSXm8TACbaj+fM7CAAXjJVCHFFclGfB83sKgA3AXiyPXSvmR0ws/vNjLcWFUKsOysOdjPrA/ADAJ9z91kAXwdwDYA9aN35v0zm3WNm+81sf32Jt0oWQqwtKwp2a1Xh/wGA77j7DwHA3U+7e8PdmwC+ASDYYN3d97n7Xnffm+/mjSCEEGvLssFuZgbgmwAOuvtXLhgfv+DXPgGAb0kLIdadlezG/y6AzwB40cyeb499HsCnzWwPWnLcUQB/sdyBzIzKJIUCl5rq5bCccPT0LJ1TWThIbbfdfB21dQ+NU9tMOSyR/POT++mcsvPMpVqdyzilEs9sa0bqoC0uhlsJxcgiGVnGk94Q6ciEEpG8YllZiNisxGXK7m5euy5PpL5aJKNsbmGB2hoRmbJS56/L4HC4jiIAjI2HbX2RwntLc+E/iT1ybaxkN/5nAEIveVRTF0JcWegbdEIkgoJdiERQsAuRCAp2IRJBwS5EInS04CTc0ayTLKpYxlAWlqGq4NlOk/MVanv2NV7o8c5FLq3MeVjuOHGefzOw1Mezq+qL3P9yhfvf0xORmkjbq9jxLMf9yEXaNcUy2JzIaB65vxQicuN8jWffVetcKmOyXCxjLyahLURab/UNcXltaCNvOVath4/52qs8q7NAshFrVe6f7uxCJIKCXYhEULALkQgKdiESQcEuRCIo2IVIhA5LbwBY1pBzuSPLwsX6ms5loUaOF/g7Osmlsvsf4vk9H719b3D8yMkzwXEAWGzEihBGZKguXjgwK3JbD+lhVuzmstbSHJeuYtlhHpGoCiRjK8vz1yx2rixSVDLWx25pcf6i58TONTQ8Qm0bxnjG5NlzU9Q2ffZUePwt3pPw2l27woaIpKg7uxCJoGAXIhEU7EIkgoJdiERQsAuRCAp2IRKho9Jbls8wMjQUtJXLXA5bWApn8hQznv1Vj8hCuUhxyyeeOkBtR06Gs+VmFnjhyKn5JWojyU4AgN7eSLZcpKhgqRR+bvmIXNfVzTPKskhGXL7Aj9kg95F6RPKyiM2d+9io8fWv1sKL3N3FpcjRDRuobXiUy2vVSOZmpRgpHkn6szXzXD5eKIevq2ZEwtadXYhEULALkQgKdiESQcEuRCIo2IVIhGV3482sC8ATAErt3/++u3/BzHYBeBDABgDPAPiMu0f2lwFvOipkF7EUedupNMK7rYWM7wbX+SYyPMdPluvmu+DHSMJLLpLcUa/xHeaYYlAul6ltIdKeKEeeG9ulB4DeIt/17Y4k0ORy3P9iV/h83T18fatVnghzdoonkjTB5+UL4fUYHuilc8ZGwooRAGzezBNhphd4nb+56fPUNj8zHRwfGuHnOnvmbHC8HkkmWsmdvQLgo+7+AbTaM99hZh8C8LcAvuru1wI4D+CzKziWEGKdWDbYvcU7eYKF9j8H8FEA32+PPwDg42vioRDisrDS/uxZu4PrJIAfA3gTwLT7L1uUHgewdW1cFEJcDlYU7O7ecPc9ALYBuAXA9Ss9gZndY2b7zWx/bZG3WBZCrC0XtRvv7tMA/gnAhwEMmf2ysfc2ACfInH3uvtfd9xZ6BlblrBDi0lk22M1so5kNtR93A/gYgINoBf2ftn/tbgCPrJWTQojVs5JEmHEAD5hZhtabw0Pu/vdm9gqAB83sPwN4DsA3lztQs9lEZSksKZUyo/N6iJfNGk8yiXQtQhNcMoolEjRJu6l6NZLA0eDPK9aCKGZrRhJhmPR2/jyXfqYi6zjQxyWqwUg9tgFSC68LXMprNLl0lbdIsk6Jv9iVcviYpTx/XWLnqi/ORGzc//npc9TWJMk6XSUuiZZZnTyLPC9qaePuBwDcFBg/jNbf70KIXwH0DTohEkHBLkQiKNiFSAQFuxCJoGAXIhEsJvFc9pOZnQFwrP3jKIBw6k5nkR/vRn68m181P3a6+8aQoaPB/q4Tm+1393DzNPkhP+THZfdDH+OFSAQFuxCJsJ7Bvm8dz30h8uPdyI9382vjx7r9zS6E6Cz6GC9EIqxLsJvZHWb2mpkdMrP71sOHth9HzexFM3vezPZ38Lz3m9mkmb10wdiImf3YzN5o/z+8Tn580cxOtNfkeTO7swN+bDezfzKzV8zsZTP76/Z4R9ck4kdH18TMuszsKTN7oe3Hf2qP7zKzJ9tx8z0z4xVXQ7h7R/8ByNAqa3U1gCKAFwDc0Gk/2r4cBTC6Due9DcDNAF66YOy/ALiv/fg+AH+7Tn58EcC/7/B6jAO4uf24H8DrAG7o9JpE/OjomgAwAH3txwUATwL4EICHAHyqPf7fAfzlxRx3Pe7stwA45O6HvVV6+kEAd62DH+uGuz8B4L21ke9Cq3An0KECnsSPjuPuE+7+bPvxHFrFUbaiw2sS8aOjeIvLXuR1PYJ9K4C3L/h5PYtVOoAfmdkzZnbPOvnwDmPuPtF+fArA2Dr6cq+ZHWh/zF/zPycuxMyuQqt+wpNYxzV5jx9Ah9dkLYq8pr5Bd6u73wzgDwH8lZndtt4OAa13drTeiNaDrwO4Bq0eARMAvtypE5tZH4AfAPicu7+rOmkn1yTgR8fXxFdR5JWxHsF+AsD2C36mxSrXGnc/0f5/EsDDWN/KO6fNbBwA2v9ProcT7n66faE1AXwDHVoTMyugFWDfcfcftoc7viYhP9ZrTdrnvugir4z1CPanAexu7ywWAXwKwKOddsLMes2s/53HAP4AwEvxWWvKo2gV7gTWsYDnO8HV5hPowJqYmaFVw/Cgu3/lAlNH14T50ek1WbMir53aYXzPbuOdaO10vgngP6yTD1ejpQS8AODlTvoB4LtofRysofW312fR6pn3OIA3APwEwMg6+fFtAC8COIBWsI13wI9b0fqIfgDA8+1/d3Z6TSJ+dHRNAPwWWkVcD6D1xvIfL7hmnwJwCMD/BlC6mOPqG3RCJELqG3RCJIOCXYhEULALkQgKdiESQcEuRCIo2IVIBAW7EImgYBciEf4vt7E0CnHQV6IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "def imshow(img):\n",
    "    # Unnormalize line not needed because there is no normalization applied\n",
    "    #img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    print(npimg.shape)\n",
    "    npimg_transpose = np.transpose(npimg, (1, 2, 0))\n",
    "    print(npimg_transpose.shape)\n",
    "    plt.imshow(npimg_transpose)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "examples = iter(test_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "image = example_data[0]\n",
    "imshow(image)\n",
    "\n",
    "#npimg = image.numpy()\n",
    "#npimg = npimg.reshape(32, 32, 3) # DOES NOT WORK!\n",
    "#npimg = np.transpose(npimg, (1, 2, 0)) # WORKS\n",
    "# Using transpose instead of reshape actually help to display the image correctly\n",
    "#print(npimg.shape)\n",
    "#plt.imshow(npimg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test data shapes from loader\n",
    "\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print(i)\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    images = images.reshape(-1, 3*32*32)\n",
    "    print(images.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1) DESIGN MODEL\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out = self.l1(x)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.l2(out)\n",
    "#         # no activation and no softmax at the end\n",
    "#         return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # TODO is this the correct way to call softmax? with dim = -1 ??? YES. From Unsupervised model code.\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "    \n",
    "print(f'Created model with input_size_cifar_rgb:{input_size_cifar_rgb}, hidden_size:{hidden_size}, num_classes:{num_classes}')\n",
    "\n",
    "#model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "#model = NeuralNet(input_size_mnist, hidden_size, num_classes).to(device)\n",
    "model = NeuralNet(input_size_cifar_rgb, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model with input_size_cifar_rgb:3072, hidden_size:2000, num_classes:10\n"
     ]
    }
   ],
   "source": [
    "# 2.2) DESIGN MODEL with custom relu and custom tanh\n",
    "# Fully connected neural network with one hidden layer and using custom RELU and custom tanh activation functions\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, n=1, beta=.01):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.n = n\n",
    "        self.beta = beta\n",
    "        self.l1 = nn.Linear(input_size, hidden_size, bias=False) \n",
    "        self.l2 = nn.Linear(hidden_size, num_classes, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = F.relu(out) ** self.n\n",
    "        out = self.l2(out)\n",
    "        out = torch.tanh(self.beta * out)\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "print(f'Created model with input_size_cifar_rgb:{input_size_cifar_rgb}, hidden_size:{hidden_size}, num_classes:{num_classes}')\n",
    "\n",
    "#model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "#model = NeuralNet(input_size_mnist, hidden_size, num_classes).to(device)\n",
    "model = NeuralNet(input_size_cifar_rgb, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE: https://github.com/gatapia/unsupervised_bio_classifier\n",
    "# Custom loss function based off \"Competing Hidden Units\" paper\n",
    "\n",
    "class BioLoss(nn.Module):\n",
    "    def __init__(self, m=4):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "\n",
    "    # According to the hidden units paper, c = predictions, t = actual labels\n",
    "    def forward(self, c, t):\n",
    "        t_ohe = torch.eye(10, dtype=torch.float, device='cuda')[t]\n",
    "        t_ohe[t_ohe==0] = -1.        \n",
    "        loss = (c - t_ohe).abs() ** self.m\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_group[lr]: 0.004\n"
     ]
    }
   ],
   "source": [
    "# 3) LOSS AND OPTIMIZER\n",
    "\n",
    "# TODO: Use the custom loss function in th paper. Equation [12]. It seems to use one-hot encoded labels.\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "criterion = BioLoss(m=6)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "#print(optimizer.param_groups)\n",
    "\n",
    "# TODO: To dynamically change learning rate\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(\"param_group[lr]:\", param_group['lr'])\n",
    "#    param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) TRAINING LOOP for testing normalization pre-processing works properly\n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "print(f'n_total_steps: {n_total_steps}')\n",
    "\n",
    "total_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "print(f'total_samples: {total_samples}, n_iterations: {n_iterations}')\n",
    "\n",
    "'''\n",
    "# Dummy training loop for printing \n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        \n",
    "        # here: 60000 samples, batch_size = 100, n_iters=60000/100 = 600 iterations\n",
    "        # Run your training process\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{n_iterations}| Inputs {images.shape} | Labels {labels.shape}')\n",
    "'''\n",
    "\n",
    "# Real training loop\n",
    "for epoch in range(num_epochs):\n",
    "#     if epoch == 50:\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             print(\"param_group[lr] before:\", param_group['lr'])\n",
    "#             param_group['lr'] = 0.001\n",
    "#             print(\"param_group[lr] after:\", param_group['lr'])\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        \n",
    "        images = images.reshape(-1, 3*32*32)\n",
    "        \n",
    "        print(f'i: {i}')\n",
    "        print(f'images.shape: {images.shape}')\n",
    "        print()\n",
    "        \n",
    "        sample_data = images[0].numpy()\n",
    "        print(f'Magnitude of unnormalized images sample_data: {np.linalg.norm(sample_data)}')\n",
    "        print(sample_data)\n",
    "        print(f'Min value in images sample_data: {np.amin(sample_data)}')\n",
    "        print(f'Max value in images sample_data: {np.amax(sample_data)}')\n",
    "        print()\n",
    "        \n",
    "        # PreProcessing\n",
    "        # NORMALIZE EACH IMAGE to unit vector\n",
    "        \n",
    "        # NORMALIZATION as per the \"Hidden Competing Units\" paper\n",
    "        # L2-normalize the training samples to unit vectors.\n",
    "        # data_cifar = preprocessing.normalize(images, norm='l2')\n",
    "        images_normalized = preprocessing.normalize(images, norm='l2')\n",
    "        print(f'images_normalized shape: {images_normalized.shape}')\n",
    "        print(f'Number of CIFAR-10 images_normalized training samples: {images_normalized.shape[0]}')\n",
    "        print(f'Number of CIFAR-10 images_normalized features: {images_normalized.shape[1]}')\n",
    "\n",
    "        # Test print normalized data\n",
    "        sample_data = images_normalized[0]\n",
    "        print(f'sample_data.shape: {sample_data.shape}')\n",
    "        print(f'Magnitude of the normalized vector sample_data: {np.linalg.norm(sample_data)}')\n",
    "        print(f'Min value in images_normalized sample_data: {np.amin(sample_data)}')\n",
    "        print(f'Max value in images_normalized sample_data: {np.amax(sample_data)}')\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        # NORMALIZATION with PyTorch\n",
    "        images_normalized_torch = F.normalize(images, p=2, dim=1)\n",
    "        print(f'images_normalized_torch shape: {images_normalized_torch.shape}')\n",
    "        print(f'Number of CIFAR-10 images_normalized_torch training samples: {images_normalized_torch.shape[0]}')\n",
    "        print(f'Number of CIFAR-10 images_normalized_torch features: {images_normalized_torch.shape[1]}')\n",
    "        \n",
    "        sample_data = images_normalized_torch[0].numpy()\n",
    "        print(f'images_normalized_torch sample_data shape: {sample_data.shape}')\n",
    "        print(f'Magnitude of the normalized images_normalized_torch sample_data vector: {np.linalg.norm(sample_data)}')\n",
    "        print(f'Max value in images_normalized_torch sample_data: {np.amax(sample_data)}')\n",
    "        print(f'Min value in images_normalized_torch sample_data: {np.amin(sample_data)}')\n",
    "        print()\n",
    "        \n",
    "        # Test print normalized data\n",
    "        #sample_data = data_cifar[0]\n",
    "        #sample_data = images_normalized[0]\n",
    "        #print(f'sample_data.shape: {sample_data.shape}')\n",
    "        #print(f'Magnitude of the normalized vector: {np.linalg.norm(sample_data)}')\n",
    "        #print(sample_data)\n",
    "        #print(f'Max value in images_normalized: {np.amax(images_normalized)}')\n",
    "        #print(f'Min value in images_normalized: {np.amin(images_normalized)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_total_steps: 500\n",
      "total_samples: 50000, n_iterations: 500\n",
      "Epoch [1/100], Step [100/500], Loss: 118867.0000\n",
      "Epoch [1/100], Step [200/500], Loss: 110998.9375\n",
      "Epoch [1/100], Step [300/500], Loss: 108447.1406\n",
      "Epoch [1/100], Step [400/500], Loss: 107723.9297\n",
      "Epoch [1/100], Step [500/500], Loss: 99152.1094\n",
      "Epoch [2/100], Step [100/500], Loss: 104113.5625\n",
      "Epoch [2/100], Step [200/500], Loss: 96195.3750\n",
      "Epoch [2/100], Step [300/500], Loss: 92915.7109\n",
      "Epoch [2/100], Step [400/500], Loss: 101780.7344\n",
      "Epoch [2/100], Step [500/500], Loss: 91103.0938\n",
      "Epoch [3/100], Step [100/500], Loss: 92232.5234\n",
      "Epoch [3/100], Step [200/500], Loss: 90689.7500\n",
      "Epoch [3/100], Step [300/500], Loss: 95630.0625\n",
      "Epoch [3/100], Step [400/500], Loss: 91910.8906\n",
      "Epoch [3/100], Step [500/500], Loss: 95187.9922\n",
      "Epoch [4/100], Step [100/500], Loss: 85391.3594\n",
      "Epoch [4/100], Step [200/500], Loss: 91870.6406\n",
      "Epoch [4/100], Step [300/500], Loss: 90252.3828\n",
      "Epoch [4/100], Step [400/500], Loss: 85556.9531\n",
      "Epoch [4/100], Step [500/500], Loss: 84550.7812\n",
      "Epoch [5/100], Step [100/500], Loss: 89253.7500\n",
      "Epoch [5/100], Step [200/500], Loss: 86369.6953\n",
      "Epoch [5/100], Step [300/500], Loss: 87975.3125\n",
      "Epoch [5/100], Step [400/500], Loss: 84100.4453\n",
      "Epoch [5/100], Step [500/500], Loss: 89257.5469\n",
      "Epoch [6/100], Step [100/500], Loss: 84436.6484\n",
      "Epoch [6/100], Step [200/500], Loss: 89136.2656\n",
      "Epoch [6/100], Step [300/500], Loss: 78115.6875\n",
      "Epoch [6/100], Step [400/500], Loss: 83193.2578\n",
      "Epoch [6/100], Step [500/500], Loss: 85646.1094\n",
      "Epoch [7/100], Step [100/500], Loss: 76897.4375\n",
      "Epoch [7/100], Step [200/500], Loss: 81699.8281\n",
      "Epoch [7/100], Step [300/500], Loss: 78384.3750\n",
      "Epoch [7/100], Step [400/500], Loss: 73735.5938\n",
      "Epoch [7/100], Step [500/500], Loss: 83999.7188\n",
      "Epoch [8/100], Step [100/500], Loss: 84175.6953\n",
      "Epoch [8/100], Step [200/500], Loss: 87709.2500\n",
      "Epoch [8/100], Step [300/500], Loss: 81908.9922\n",
      "Epoch [8/100], Step [400/500], Loss: 82290.4609\n",
      "Epoch [8/100], Step [500/500], Loss: 82138.0078\n",
      "Epoch [9/100], Step [100/500], Loss: 74899.6250\n",
      "Epoch [9/100], Step [200/500], Loss: 92661.7500\n",
      "Epoch [9/100], Step [300/500], Loss: 85206.6562\n",
      "Epoch [9/100], Step [400/500], Loss: 79166.9141\n",
      "Epoch [9/100], Step [500/500], Loss: 75085.1016\n",
      "Epoch [10/100], Step [100/500], Loss: 80348.2969\n",
      "Epoch [10/100], Step [200/500], Loss: 85085.2188\n",
      "Epoch [10/100], Step [300/500], Loss: 82116.4844\n",
      "Epoch [10/100], Step [400/500], Loss: 83969.4531\n",
      "Epoch [10/100], Step [500/500], Loss: 86717.2344\n",
      "Epoch [11/100], Step [100/500], Loss: 74416.9375\n",
      "Epoch [11/100], Step [200/500], Loss: 87293.5625\n",
      "Epoch [11/100], Step [300/500], Loss: 82405.3750\n",
      "Epoch [11/100], Step [400/500], Loss: 80899.4141\n",
      "Epoch [11/100], Step [500/500], Loss: 75280.1562\n",
      "Epoch [12/100], Step [100/500], Loss: 86481.0000\n",
      "Epoch [12/100], Step [200/500], Loss: 86005.4531\n",
      "Epoch [12/100], Step [300/500], Loss: 83087.0547\n",
      "Epoch [12/100], Step [400/500], Loss: 74454.7031\n",
      "Epoch [12/100], Step [500/500], Loss: 77951.8594\n",
      "Epoch [13/100], Step [100/500], Loss: 75668.5312\n",
      "Epoch [13/100], Step [200/500], Loss: 90735.8281\n",
      "Epoch [13/100], Step [300/500], Loss: 76166.0234\n",
      "Epoch [13/100], Step [400/500], Loss: 80334.3438\n",
      "Epoch [13/100], Step [500/500], Loss: 85860.7109\n",
      "Epoch [14/100], Step [100/500], Loss: 75464.6719\n",
      "Epoch [14/100], Step [200/500], Loss: 79409.5234\n",
      "Epoch [14/100], Step [300/500], Loss: 77889.4766\n",
      "Epoch [14/100], Step [400/500], Loss: 82097.9453\n",
      "Epoch [14/100], Step [500/500], Loss: 74360.3984\n",
      "Epoch [15/100], Step [100/500], Loss: 76439.8750\n",
      "Epoch [15/100], Step [200/500], Loss: 71479.3750\n",
      "Epoch [15/100], Step [300/500], Loss: 81658.0000\n",
      "Epoch [15/100], Step [400/500], Loss: 77386.8750\n",
      "Epoch [15/100], Step [500/500], Loss: 90292.2344\n",
      "Epoch [16/100], Step [100/500], Loss: 80201.7031\n",
      "Epoch [16/100], Step [200/500], Loss: 71713.2266\n",
      "Epoch [16/100], Step [300/500], Loss: 76759.0625\n",
      "Epoch [16/100], Step [400/500], Loss: 74150.4062\n",
      "Epoch [16/100], Step [500/500], Loss: 77344.2266\n",
      "Epoch [17/100], Step [100/500], Loss: 72236.9453\n",
      "Epoch [17/100], Step [200/500], Loss: 78161.4688\n",
      "Epoch [17/100], Step [300/500], Loss: 73703.7656\n",
      "Epoch [17/100], Step [400/500], Loss: 79109.4766\n",
      "Epoch [17/100], Step [500/500], Loss: 65032.5117\n",
      "Epoch [18/100], Step [100/500], Loss: 71571.8438\n",
      "Epoch [18/100], Step [200/500], Loss: 69889.6875\n",
      "Epoch [18/100], Step [300/500], Loss: 73282.7500\n",
      "Epoch [18/100], Step [400/500], Loss: 85660.7812\n",
      "Epoch [18/100], Step [500/500], Loss: 74885.1719\n",
      "Epoch [19/100], Step [100/500], Loss: 70817.1953\n",
      "Epoch [19/100], Step [200/500], Loss: 79993.3438\n",
      "Epoch [19/100], Step [300/500], Loss: 75311.9062\n",
      "Epoch [19/100], Step [400/500], Loss: 71351.2656\n",
      "Epoch [19/100], Step [500/500], Loss: 72410.9141\n",
      "Epoch [20/100], Step [100/500], Loss: 67449.4688\n",
      "Epoch [20/100], Step [200/500], Loss: 76325.8750\n",
      "Epoch [20/100], Step [300/500], Loss: 72667.9531\n",
      "Epoch [20/100], Step [400/500], Loss: 69994.8281\n",
      "Epoch [20/100], Step [500/500], Loss: 69374.1094\n",
      "Epoch [21/100], Step [100/500], Loss: 70895.9688\n",
      "Epoch [21/100], Step [200/500], Loss: 68696.8906\n",
      "Epoch [21/100], Step [300/500], Loss: 72502.7500\n",
      "Epoch [21/100], Step [400/500], Loss: 71022.8516\n",
      "Epoch [21/100], Step [500/500], Loss: 69619.8125\n",
      "Epoch [22/100], Step [100/500], Loss: 69489.5000\n",
      "Epoch [22/100], Step [200/500], Loss: 75278.4844\n",
      "Epoch [22/100], Step [300/500], Loss: 69910.9219\n",
      "Epoch [22/100], Step [400/500], Loss: 70276.8750\n",
      "Epoch [22/100], Step [500/500], Loss: 70642.2031\n",
      "Epoch [23/100], Step [100/500], Loss: 71758.9062\n",
      "Epoch [23/100], Step [200/500], Loss: 70426.7969\n",
      "Epoch [23/100], Step [300/500], Loss: 77207.6094\n",
      "Epoch [23/100], Step [400/500], Loss: 77133.5859\n",
      "Epoch [23/100], Step [500/500], Loss: 70271.3906\n",
      "Epoch [24/100], Step [100/500], Loss: 62535.3281\n",
      "Epoch [24/100], Step [200/500], Loss: 76220.1719\n",
      "Epoch [24/100], Step [300/500], Loss: 78358.7656\n",
      "Epoch [24/100], Step [400/500], Loss: 71984.8906\n",
      "Epoch [24/100], Step [500/500], Loss: 63929.1094\n",
      "Epoch [25/100], Step [100/500], Loss: 71460.3672\n",
      "Epoch [25/100], Step [200/500], Loss: 70615.5156\n",
      "Epoch [25/100], Step [300/500], Loss: 72738.1016\n",
      "Epoch [25/100], Step [400/500], Loss: 70530.8906\n",
      "Epoch [25/100], Step [500/500], Loss: 82818.8750\n",
      "Epoch [26/100], Step [100/500], Loss: 73123.3125\n",
      "Epoch [26/100], Step [200/500], Loss: 67269.9844\n",
      "Epoch [26/100], Step [300/500], Loss: 73012.0625\n",
      "Epoch [26/100], Step [400/500], Loss: 70287.1562\n",
      "Epoch [26/100], Step [500/500], Loss: 64210.5742\n",
      "Epoch [27/100], Step [100/500], Loss: 69759.9531\n",
      "Epoch [27/100], Step [200/500], Loss: 63096.7344\n",
      "Epoch [27/100], Step [300/500], Loss: 68269.9375\n",
      "Epoch [27/100], Step [400/500], Loss: 74537.4531\n",
      "Epoch [27/100], Step [500/500], Loss: 67616.7656\n",
      "Epoch [28/100], Step [100/500], Loss: 64707.5664\n",
      "Epoch [28/100], Step [200/500], Loss: 58356.3359\n",
      "Epoch [28/100], Step [300/500], Loss: 69856.0469\n",
      "Epoch [28/100], Step [400/500], Loss: 72014.5781\n",
      "Epoch [28/100], Step [500/500], Loss: 71516.1016\n",
      "Epoch [29/100], Step [100/500], Loss: 61181.6133\n",
      "Epoch [29/100], Step [200/500], Loss: 66248.4531\n",
      "Epoch [29/100], Step [300/500], Loss: 65298.0859\n",
      "Epoch [29/100], Step [400/500], Loss: 64498.2617\n",
      "Epoch [29/100], Step [500/500], Loss: 73569.2266\n",
      "Epoch [30/100], Step [100/500], Loss: 67484.3750\n",
      "Epoch [30/100], Step [200/500], Loss: 64785.5664\n",
      "Epoch [30/100], Step [300/500], Loss: 69842.5859\n",
      "Epoch [30/100], Step [400/500], Loss: 63029.1641\n",
      "Epoch [30/100], Step [500/500], Loss: 57984.1016\n",
      "Epoch [31/100], Step [100/500], Loss: 72162.0156\n",
      "Epoch [31/100], Step [200/500], Loss: 63778.3359\n",
      "Epoch [31/100], Step [300/500], Loss: 65152.1367\n",
      "Epoch [31/100], Step [400/500], Loss: 66027.4688\n",
      "Epoch [31/100], Step [500/500], Loss: 69245.4531\n",
      "Epoch [32/100], Step [100/500], Loss: 65284.2773\n",
      "Epoch [32/100], Step [200/500], Loss: 58532.4375\n",
      "Epoch [32/100], Step [300/500], Loss: 75495.6875\n",
      "Epoch [32/100], Step [400/500], Loss: 62074.0352\n",
      "Epoch [32/100], Step [500/500], Loss: 62597.3477\n",
      "Epoch [33/100], Step [100/500], Loss: 67980.3047\n",
      "Epoch [33/100], Step [200/500], Loss: 64124.2109\n",
      "Epoch [33/100], Step [300/500], Loss: 65170.3984\n",
      "Epoch [33/100], Step [400/500], Loss: 65952.4609\n",
      "Epoch [33/100], Step [500/500], Loss: 59744.0703\n",
      "Epoch [34/100], Step [100/500], Loss: 64747.9180\n",
      "Epoch [34/100], Step [200/500], Loss: 62049.7109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Step [300/500], Loss: 69005.4062\n",
      "Epoch [34/100], Step [400/500], Loss: 61194.2422\n",
      "Epoch [34/100], Step [500/500], Loss: 68735.2422\n",
      "Epoch [35/100], Step [100/500], Loss: 59374.7930\n",
      "Epoch [35/100], Step [200/500], Loss: 65597.2891\n",
      "Epoch [35/100], Step [300/500], Loss: 62663.9336\n",
      "Epoch [35/100], Step [400/500], Loss: 66922.3984\n",
      "Epoch [35/100], Step [500/500], Loss: 65335.6641\n",
      "Epoch [36/100], Step [100/500], Loss: 69647.1562\n",
      "Epoch [36/100], Step [200/500], Loss: 60554.8047\n",
      "Epoch [36/100], Step [300/500], Loss: 65883.2656\n",
      "Epoch [36/100], Step [400/500], Loss: 69709.0469\n",
      "Epoch [36/100], Step [500/500], Loss: 67404.0391\n",
      "Epoch [37/100], Step [100/500], Loss: 65353.3516\n",
      "Epoch [37/100], Step [200/500], Loss: 60130.6328\n",
      "Epoch [37/100], Step [300/500], Loss: 55827.0156\n",
      "Epoch [37/100], Step [400/500], Loss: 71033.7031\n",
      "Epoch [37/100], Step [500/500], Loss: 78470.2422\n",
      "Epoch [38/100], Step [100/500], Loss: 66215.2969\n",
      "Epoch [38/100], Step [200/500], Loss: 60896.0234\n",
      "Epoch [38/100], Step [300/500], Loss: 61181.7773\n",
      "Epoch [38/100], Step [400/500], Loss: 57858.7891\n",
      "Epoch [38/100], Step [500/500], Loss: 56156.4648\n",
      "Epoch [39/100], Step [100/500], Loss: 56834.2578\n",
      "Epoch [39/100], Step [200/500], Loss: 63765.4609\n",
      "Epoch [39/100], Step [300/500], Loss: 62754.0195\n",
      "Epoch [39/100], Step [400/500], Loss: 67637.3828\n",
      "Epoch [39/100], Step [500/500], Loss: 60723.0625\n",
      "Epoch [40/100], Step [100/500], Loss: 58410.1094\n",
      "Epoch [40/100], Step [200/500], Loss: 53554.2812\n",
      "Epoch [40/100], Step [300/500], Loss: 72046.1250\n",
      "Epoch [40/100], Step [400/500], Loss: 63400.5703\n",
      "Epoch [40/100], Step [500/500], Loss: 54878.4531\n",
      "Epoch [41/100], Step [100/500], Loss: 60995.9102\n",
      "Epoch [41/100], Step [200/500], Loss: 59992.3203\n",
      "Epoch [41/100], Step [300/500], Loss: 60135.4102\n",
      "Epoch [41/100], Step [400/500], Loss: 64058.5195\n",
      "Epoch [41/100], Step [500/500], Loss: 59503.3906\n",
      "Epoch [42/100], Step [100/500], Loss: 55434.9688\n",
      "Epoch [42/100], Step [200/500], Loss: 65939.8047\n",
      "Epoch [42/100], Step [300/500], Loss: 57235.7578\n",
      "Epoch [42/100], Step [400/500], Loss: 53405.2617\n",
      "Epoch [42/100], Step [500/500], Loss: 56148.9648\n",
      "Epoch [43/100], Step [100/500], Loss: 60203.1406\n",
      "Epoch [43/100], Step [200/500], Loss: 61070.9453\n",
      "Epoch [43/100], Step [300/500], Loss: 61525.1133\n",
      "Epoch [43/100], Step [400/500], Loss: 61823.9805\n",
      "Epoch [43/100], Step [500/500], Loss: 64308.1055\n",
      "Epoch [44/100], Step [100/500], Loss: 58566.4531\n",
      "Epoch [44/100], Step [200/500], Loss: 57579.1211\n",
      "Epoch [44/100], Step [300/500], Loss: 58473.7656\n",
      "Epoch [44/100], Step [400/500], Loss: 59731.3594\n",
      "Epoch [44/100], Step [500/500], Loss: 59853.6562\n",
      "Epoch [45/100], Step [100/500], Loss: 52758.5938\n",
      "Epoch [45/100], Step [200/500], Loss: 61535.9375\n",
      "Epoch [45/100], Step [300/500], Loss: 53849.1523\n",
      "Epoch [45/100], Step [400/500], Loss: 57119.5859\n",
      "Epoch [45/100], Step [500/500], Loss: 59001.5117\n",
      "Epoch [46/100], Step [100/500], Loss: 55751.6367\n",
      "Epoch [46/100], Step [200/500], Loss: 58989.5039\n",
      "Epoch [46/100], Step [300/500], Loss: 59177.4688\n",
      "Epoch [46/100], Step [400/500], Loss: 58382.0703\n",
      "Epoch [46/100], Step [500/500], Loss: 61533.0859\n",
      "Epoch [47/100], Step [100/500], Loss: 61131.9297\n",
      "Epoch [47/100], Step [200/500], Loss: 61479.0117\n",
      "Epoch [47/100], Step [300/500], Loss: 52866.8477\n",
      "Epoch [47/100], Step [400/500], Loss: 59673.1133\n",
      "Epoch [47/100], Step [500/500], Loss: 60487.1953\n",
      "Epoch [48/100], Step [100/500], Loss: 51740.3242\n",
      "Epoch [48/100], Step [200/500], Loss: 53928.8359\n",
      "Epoch [48/100], Step [300/500], Loss: 53709.7656\n",
      "Epoch [48/100], Step [400/500], Loss: 53282.0469\n",
      "Epoch [48/100], Step [500/500], Loss: 60995.5781\n",
      "Epoch [49/100], Step [100/500], Loss: 60347.8320\n",
      "Epoch [49/100], Step [200/500], Loss: 60575.3008\n",
      "Epoch [49/100], Step [300/500], Loss: 56036.5273\n",
      "Epoch [49/100], Step [400/500], Loss: 58872.5234\n",
      "Epoch [49/100], Step [500/500], Loss: 56118.5547\n",
      "Epoch [50/100], Step [100/500], Loss: 56809.2773\n",
      "Epoch [50/100], Step [200/500], Loss: 51602.3945\n",
      "Epoch [50/100], Step [300/500], Loss: 59296.0469\n",
      "Epoch [50/100], Step [400/500], Loss: 52844.2031\n",
      "Epoch [50/100], Step [500/500], Loss: 59063.0703\n",
      "param_group[lr] before: 0.004\n",
      "param_group[lr] after: 0.001\n",
      "Epoch [51/100], Step [100/500], Loss: 54939.7812\n",
      "Epoch [51/100], Step [200/500], Loss: 51036.0156\n",
      "Epoch [51/100], Step [300/500], Loss: 55751.9453\n",
      "Epoch [51/100], Step [400/500], Loss: 53643.8164\n",
      "Epoch [51/100], Step [500/500], Loss: 56317.6484\n",
      "Epoch [52/100], Step [100/500], Loss: 52742.6719\n",
      "Epoch [52/100], Step [200/500], Loss: 53193.6992\n",
      "Epoch [52/100], Step [300/500], Loss: 53617.5312\n",
      "Epoch [52/100], Step [400/500], Loss: 56617.2969\n",
      "Epoch [52/100], Step [500/500], Loss: 55553.7031\n",
      "Epoch [53/100], Step [100/500], Loss: 56586.0742\n",
      "Epoch [53/100], Step [200/500], Loss: 54084.4805\n",
      "Epoch [53/100], Step [300/500], Loss: 51506.2734\n",
      "Epoch [53/100], Step [400/500], Loss: 49865.2344\n",
      "Epoch [53/100], Step [500/500], Loss: 56010.1562\n",
      "Epoch [54/100], Step [100/500], Loss: 50560.7656\n",
      "Epoch [54/100], Step [200/500], Loss: 50427.5703\n",
      "Epoch [54/100], Step [300/500], Loss: 55536.2188\n",
      "Epoch [54/100], Step [400/500], Loss: 54724.4258\n",
      "Epoch [54/100], Step [500/500], Loss: 54165.4492\n",
      "Epoch [55/100], Step [100/500], Loss: 59824.5234\n",
      "Epoch [55/100], Step [200/500], Loss: 58299.8828\n",
      "Epoch [55/100], Step [300/500], Loss: 54312.1172\n",
      "Epoch [55/100], Step [400/500], Loss: 51492.2109\n",
      "Epoch [55/100], Step [500/500], Loss: 56358.4141\n",
      "Epoch [56/100], Step [100/500], Loss: 51495.8438\n",
      "Epoch [56/100], Step [200/500], Loss: 53949.6602\n",
      "Epoch [56/100], Step [300/500], Loss: 53167.1094\n",
      "Epoch [56/100], Step [400/500], Loss: 50552.2500\n",
      "Epoch [56/100], Step [500/500], Loss: 53342.0781\n",
      "Epoch [57/100], Step [100/500], Loss: 53904.6719\n",
      "Epoch [57/100], Step [200/500], Loss: 55741.6641\n",
      "Epoch [57/100], Step [300/500], Loss: 59153.1836\n",
      "Epoch [57/100], Step [400/500], Loss: 60359.5312\n"
     ]
    }
   ],
   "source": [
    "# 4) TRAINING LOOP with Normalization\n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "print(f'n_total_steps: {n_total_steps}')\n",
    "\n",
    "total_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "print(f'total_samples: {total_samples}, n_iterations: {n_iterations}')\n",
    "\n",
    "# Real training loop\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch == 50:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(\"param_group[lr] before:\", param_group['lr'])\n",
    "            param_group['lr'] = 0.001\n",
    "            print(\"param_group[lr] after:\", param_group['lr'])\n",
    "            \n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "                \n",
    "        # original shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        #images = images.reshape(-1, 28*28).to(device)\n",
    "        images = images.reshape(-1, 3*32*32).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # PreProcessing\n",
    "        # NORMALIZE EACH IMAGE to unit vector\n",
    "        \n",
    "        # NORMALIZATION with Numpy\n",
    "        images_normalized = preprocessing.normalize(images.cpu().numpy(), norm='l2')\n",
    "        images_normalized = torch.from_numpy(images_normalized).to(device)\n",
    "        \n",
    "        # NORMALIZATION with PyTorch\n",
    "        images_normalized_torch = F.normalize(images, p=2, dim=1)\n",
    "        images_normalized_torch = images_normalized_torch.to(device)\n",
    "        \n",
    "        # Test print normalized data\n",
    "        sample_data = images_normalized[0]\n",
    "        sample_data = sample_data.cpu().numpy()\n",
    "        #print(f'i: {i}')\n",
    "        #print(f'sample_data.shape: {sample_data.shape}')\n",
    "        #print(f'Magnitude of the normalized vector sample_data: {np.linalg.norm(sample_data)}')\n",
    "        #print()\n",
    "        \n",
    "        # Forward pass\n",
    "        #outputs = model(images)\n",
    "        outputs = model(images_normalized)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        # original shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        #images = images.reshape(-1, 28*28).to(device)\n",
    "        images = images.reshape(-1, 3*32*32).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # PreProcessing\n",
    "        # NORMALIZE EACH IMAGE to unit vector\n",
    "        \n",
    "        # NORMALIZATION with Numpy\n",
    "        images_normalized = preprocessing.normalize(images.cpu().numpy(), norm='l2')\n",
    "        images_normalized = torch.from_numpy(images_normalized).to(device)\n",
    "        \n",
    "        # NORMALIZATION with PyTorch\n",
    "        images_normalized_torch = F.normalize(images, p=2, dim=1)\n",
    "        images_normalized_torch = images_normalized_torch.to(device)\n",
    "        \n",
    "        #outputs = model(images)\n",
    "        outputs = model(images_normalized)\n",
    "        \n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')\n",
    "\n",
    "    \n",
    "# Grayscale results\n",
    "\n",
    "# hidden_size = 200\n",
    "# Epoch [200/200], Step [100/500], Loss: 0.9271\n",
    "# Epoch [200/200], Step [200/500], Loss: 1.0624\n",
    "# Epoch [200/200], Step [300/500], Loss: 0.8441\n",
    "# Epoch [200/200], Step [400/500], Loss: 0.9473\n",
    "# Epoch [200/200], Step [500/500], Loss: 0.9008\n",
    "# Accuracy of the network on the 10000 test images: 40.76 %\n",
    "\n",
    "# hidden_size = 2000\n",
    "# Epoch [200/200], Step [100/500], Loss: 0.6262\n",
    "# Epoch [200/200], Step [200/500], Loss: 0.6148\n",
    "# Epoch [200/200], Step [300/500], Loss: 0.7147\n",
    "# Epoch [200/200], Step [400/500], Loss: 0.7993\n",
    "# Epoch [200/200], Step [500/500], Loss: 0.7019\n",
    "# Accuracy of the network on the 10000 test images: 40.41 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4) TRAINING LOOP\n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "print(n_total_steps)\n",
    "\n",
    "total_samples = len(train_dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "'''\n",
    "# Dummy training loop for printing \n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, 28*28)\n",
    "        \n",
    "        # here: 60000 samples, batch_size = 100, n_iters=60000/100 = 600 iterations\n",
    "        # Run your training process\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{n_iterations}| Inputs {images.shape} | Labels {labels.shape}')\n",
    "'''\n",
    "\n",
    "# Real training loop\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch == 50:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(\"param_group[lr] before:\", param_group['lr'])\n",
    "            param_group['lr'] = 0.001\n",
    "            print(\"param_group[lr] after:\", param_group['lr'])\n",
    "            \n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        \n",
    "        # PreProcessing\n",
    "        # NORMALIZE EACH IMAGE to unit vector\n",
    "        \n",
    "        \n",
    "        # original shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        #images = images.reshape(-1, 28*28).to(device)\n",
    "        images = images.reshape(-1, 3*32*32).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        # original shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        #images = images.reshape(-1, 28*28).to(device)\n",
    "        images = images.reshape(-1, 3*32*32).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the 10000 test images: {acc} %')\n",
    "\n",
    "    \n",
    "# Grayscale results\n",
    "\n",
    "# hidden_size = 200\n",
    "# Epoch [200/200], Step [100/500], Loss: 0.9271\n",
    "# Epoch [200/200], Step [200/500], Loss: 1.0624\n",
    "# Epoch [200/200], Step [300/500], Loss: 0.8441\n",
    "# Epoch [200/200], Step [400/500], Loss: 0.9473\n",
    "# Epoch [200/200], Step [500/500], Loss: 0.9008\n",
    "# Accuracy of the network on the 10000 test images: 40.76 %\n",
    "\n",
    "# hidden_size = 2000\n",
    "# Epoch [200/200], Step [100/500], Loss: 0.6262\n",
    "# Epoch [200/200], Step [200/500], Loss: 0.6148\n",
    "# Epoch [200/200], Step [300/500], Loss: 0.7147\n",
    "# Epoch [200/200], Step [400/500], Loss: 0.7993\n",
    "# Epoch [200/200], Step [500/500], Loss: 0.7019\n",
    "# Accuracy of the network on the 10000 test images: 40.41 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOG\n",
    "\n",
    "for param in model.parameters():\n",
    "  print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
